{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "import praw, time\n",
    "from dotenv import load_dotenv, dotenv_values\n",
    "from requests import Session\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv('.env')\n",
    "config = dotenv_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open Reddit Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully logged in to Reddit!\n",
      "Logged in as: u/GregorybLafayetteML\n"
     ]
    }
   ],
   "source": [
    "# Create a custom session with a timeout\n",
    "session = Session()\n",
    "session.headers.update({'User-Agent': 'praw'})\n",
    "session.timeout = 10  # Set a timeout of 10 seconds\n",
    "\n",
    "# Login to Reddit using PRAW\n",
    "reddit = praw.Reddit(\n",
    "    client_id=config['CLIENT_ID'],\n",
    "    client_secret=config['CLIENT_SECRET'],\n",
    "    requestor_kwargs={\"session\": session},\n",
    "    username=config['USERNAME'],\n",
    "    password=config['PASSWORD'],\n",
    "    user_agent=\"CS470 ML Project Access by u/GregorybLafayetteML\"\n",
    ")\n",
    "\n",
    "# Test the connection\n",
    "try:\n",
    "    username = reddit.user.me()\n",
    "    print(\"Successfully logged in to Reddit!\")\n",
    "    print(f\"Logged in as: u/{username}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to log in: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Accessing Reddit Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access reddit posts, we'll need send a request with the number of post we want to get. The following example finds the top 10 hottest posts on the u/wallstreetbets subreddit. We'll show the post title, score, flair, and URL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 hot posts from r/wallstreetbets:\n",
      "Title: Introducing: WSB's First Ever Paper Trading Competition, Score: 1281, Flair: Announcements , URL: https://v.redd.it/kmfu9l676fre1\n",
      "Title: Daily Discussion Thread for March 31, 2025, Score: 318, Flair: Daily Discussion, URL: https://www.reddit.com/r/wallstreetbets/comments/1jnzl3k/daily_discussion_thread_for_march_31_2025/\n",
      "Title: China, Japan, South Korea will jointly respond to US tariffs, Chinese state media says, Score: 9375, Flair: News, URL: https://www.reddit.com/r/wallstreetbets/comments/1jo6v9m/china_japan_south_korea_will_jointly_respond_to/\n",
      "Title: Goldman Sachs sees Trump tariffs spiking inflation, stunting growth and raising recession risks, Score: 13510, Flair: News, URL: https://www.cnbc.com/2025/03/30/tariffs-to-spike-inflation-stunt-growth-and-raise-recession-risks-goldman-says-.html\n",
      "Title: Household Savings not looking good, Score: 439, Flair: Discussion, URL: https://i.redd.it/59dft5gvo1se1.jpeg\n",
      "Title: Half a mil in one position, Score: 198, Flair: YOLO, URL: https://i.redd.it/qhy18ypte2se1.jpeg\n",
      "Title: What do I do, Score: 237, Flair: Loss, URL: https://www.reddit.com/gallery/1jo74l4\n",
      "Title: Full port $56K YOLO into $547 0DTE SPY Puts, Score: 374, Flair: YOLO, URL: https://i.redd.it/fwylft5841se1.png\n",
      "Title: 200 puts (~10k): If NVDA goes to $90 this week I donate $5k to Special Olympics, Score: 1053, Flair: YOLO, URL: https://i.redd.it/3fladeuk6yre1.jpeg\n",
      "Title: A weird Hail Mary to unfk my account with 15k of loss. Wish me luck / roast me, Score: 111, Flair: Discussion, URL: https://www.reddit.com/gallery/1jo5tc5\n"
     ]
    }
   ],
   "source": [
    "top_posts = reddit.subreddit('wallstreetbets').hot(limit=10)\n",
    "print(\"Top 10 hot posts from r/wallstreetbets:\")\n",
    "for post in top_posts:\n",
    "    print(f\"Title: {post.title}, Score: {post.score}, Flair: {post.link_flair_text}, URL: {post.url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, we'll need far more than ten posts at a time. The reddit API will limit our access to 100 posts at a time. Fortunately, the api uses a ListingGenerator which allows us to access our metered connection in sequential blocks. The following example shows how we can utilize this behavior, grabbing blocks of 100 posts at a time. In our example, we'll grab blocks of posts until we reach 5000 posts or our access times out. Notice that the procedure ends early with around 750-800 posts collected. The results are sparce, because our connection either timed out or was metered down by reddit. The latter option is more likely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more posts to fetch.\n",
      "Fetched 786 posts in total.\n"
     ]
    }
   ],
   "source": [
    "# Access the subreddit\n",
    "subreddit = reddit.subreddit(\"wallstreetbets\")\n",
    "\n",
    "# Initialize variables\n",
    "batch_size = 100  # Number of posts per batch\n",
    "total_posts = 5000  # Total number of posts to fetch\n",
    "all_posts = []  # To store all the retrieved posts\n",
    "after = None  # To keep track of the last post for pagination\n",
    "\n",
    "# Fetch posts in batches\n",
    "while len(all_posts) < total_posts:\n",
    "    # Fetch the next batch of posts\n",
    "    submissions = subreddit.new(limit=batch_size, params={\"after\": after})\n",
    "    \n",
    "    batch_posts = []\n",
    "    for submission in submissions:\n",
    "        batch_posts.append({\n",
    "            \"title\": submission.title,\n",
    "            \"score\": submission.score,\n",
    "            \"url\": submission.url,\n",
    "            \"created_utc\": submission.created_utc\n",
    "        })\n",
    "\n",
    "        # Update the `after` variable with the last submission's fullname\n",
    "        after = submission.fullname\n",
    "\n",
    "    # Add the batch to the main list\n",
    "    all_posts.extend(batch_posts)\n",
    "\n",
    "    # Exit loop if no more posts are available\n",
    "    if not batch_posts:\n",
    "        print(\"No more posts to fetch.\")\n",
    "        break\n",
    "\n",
    "    # Optional delay to avoid rate limits\n",
    "    time.sleep(1)  # Adjust the delay as necessary\n",
    "\n",
    "# Process the data (example: print the total number of posts fetched)\n",
    "print(f\"Fetched {len(all_posts)} posts in total.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Analysis of Reddit Data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
