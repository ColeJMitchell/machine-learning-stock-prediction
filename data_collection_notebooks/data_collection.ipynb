{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Collecting Reddit Data from r/WallStreetBets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mqH72oV8BkP"
      },
      "source": [
        "## Part 1: Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoSNYlmE8BkQ"
      },
      "source": [
        "### Install Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load environment variables from .env file\n",
        "from dotenv import load_dotenv, dotenv_values\n",
        "load_dotenv('.env')\n",
        "config = dotenv_values()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNHmHw-T8BkQ",
        "outputId": "5682a658-2c7f-4d24-e12c-866f8ade2b85"
      },
      "outputs": [],
      "source": [
        "# Import Packages\n",
        "import praw, time, os, pyarrow\n",
        "from IPython.display import display\n",
        "from requests import Session\n",
        "import pandas as pd\n",
        "from IPython import get_ipython\n",
        "from tqdm import tqdm \n",
        "import kagglehub, kaggle\n",
        "\n",
        "\n",
        "# Get config from colab or other environment.\n",
        "def is_colab():\n",
        "    return get_ipython().__class__.__module__ == \"google.colab._shell\"\n",
        "\n",
        "if is_colab():\n",
        "    from google.colab import userdata\n",
        "    config = {}\n",
        "    config['CLIENT_SECRET'] = userdata.get('CLIENT_SECRET')\n",
        "    config['CLIENT_ID'] = userdata.get('CLIENT_ID')\n",
        "    config['NAME'] = userdata.get('NAME')\n",
        "    config['REDIRECT_URI'] = userdata.get('REDIRECT_URI')\n",
        "    config['USERNAME'] = userdata.get('USERNAME')\n",
        "    config['PASSWORD'] = userdata.get('PASSWORD')\n",
        "\n",
        "else:\n",
        "    load_dotenv('.env')\n",
        "    config = dotenv_values()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQ6nQKwq8BkR"
      },
      "source": [
        "## Part 2: Collecting Submissions from Reddit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIqcjmZA8BkR"
      },
      "source": [
        "### Open Reddit Connection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "mkZtaceY8BkR",
        "outputId": "a1b8a895-f6f3-422f-dac8-b7794d675fe2"
      },
      "outputs": [],
      "source": [
        "# Create a custom session with a timeout\n",
        "session = Session()\n",
        "session.headers.update({'User-Agent': 'praw'})\n",
        "session.timeout = 10  # Set a timeout of 10 seconds\n",
        "\n",
        "# Login to Reddit using PRAW\n",
        "reddit = praw.Reddit(\n",
        "    client_id=config['CLIENT_ID'],\n",
        "    client_secret=config['CLIENT_SECRET'],\n",
        "    requestor_kwargs={\"session\": session},\n",
        "    username=config['USERNAME'],\n",
        "    password=config['PASSWORD'],\n",
        "    user_agent=\"CS470 ML Project Access by u/GregorybLafayetteML\"\n",
        ")\n",
        "\n",
        "# Add some peripheral config data\n",
        "reddit.config.log_requests = 1\n",
        "reddit.config.store_json_result = True\n",
        "\n",
        "# Test the connection\n",
        "try:\n",
        "    username = reddit.user.me()\n",
        "    print(\"Successfully logged in to Reddit!\")\n",
        "    print(f\"Logged in as: u/{username}\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to log in: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tk6FGRfK8BkS"
      },
      "source": [
        "### Accessing Reddit Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnIgfIks8BkT"
      },
      "source": [
        "To access reddit posts, we'll need send a request with the number of post we want to get. The following example finds the top 10 hottest posts on the u/wallstreetbets subreddit. We'll show the post title, score, flair, and URL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMebUrT98BkT"
      },
      "outputs": [],
      "source": [
        "top_posts = reddit.subreddit('wallstreetbets').hot(limit=10)\n",
        "print(\"Top 10 hot posts from r/wallstreetbets:\")\n",
        "for post in top_posts:\n",
        "    print(f\"Title: {post.title}, Score: {post.score}, Flair: {post.link_flair_text}, URL: {post.url}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IxVa5Xi8BkU"
      },
      "source": [
        "For this project, we'll need far more than ten posts at a time. The reddit API will limit our access to 100 posts at a time. Fortunately, the api uses a ListingGenerator which allows us to access our metered connection in sequential blocks. The following example shows how we can utilize this behavior, grabbing blocks of 100 posts at a time. In our example, we'll grab blocks of posts until we reach 5000 posts or our access times out. Notice that the procedure ends early with around 750-800 posts collected. The results are sparce, because our connection either timed out or was metered down by reddit. The latter option is more likely."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WG2zyxFP8BkU"
      },
      "outputs": [],
      "source": [
        "# Access the subreddit\n",
        "subreddit = reddit.subreddit(\"wallstreetbets\")\n",
        "\n",
        "# Initialize variables\n",
        "batch_size = 50 # Number of posts per batch\n",
        "total_posts = 5000  # Total number of posts to fetch\n",
        "all_posts = []  # To store all the retrieved posts\n",
        "after = None  # To keep track of the last post for pagination\n",
        "\n",
        "# Fetch posts in batches\n",
        "while len(all_posts) < total_posts:\n",
        "    # Fetch the next batch of posts\n",
        "    submissions = subreddit.new(limit=batch_size, params={\"after\": after})\n",
        "\n",
        "    batch_posts = []\n",
        "    for submission in tqdm(submissions, desc=\"Storing batch of posts\", unit=\"post\"):\n",
        "        batch_posts.append(submission)\n",
        "\n",
        "        # Update the `after` variable with the last submission's fullname\n",
        "        after = submission.fullname\n",
        "\n",
        "    # Add the batch to the main list\n",
        "    all_posts.extend(batch_posts)\n",
        "\n",
        "    # Exit loop if no more posts are available\n",
        "    if not batch_posts:\n",
        "        print(\"No more posts to fetch.\")\n",
        "        break\n",
        "\n",
        "    # Optional delay to avoid rate limits\n",
        "    time.sleep(1)  # Adjust the delay as necessary\n",
        "\n",
        "# Process the data (example: print the total number of posts fetched)\n",
        "print(f\"Fetched {len(all_posts)} posts in total.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_4N4b2q8BkU"
      },
      "source": [
        "Now that we have collected a large portion of posts/submssions, we'll parse the results and construct a dataframe with this data. We're going to collect more fields from this data than we might need right now, avoiding data limitations in the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69crArfG8BkU"
      },
      "outputs": [],
      "source": [
        "# Parse are submission objects that we collected.\n",
        "fields = ('title',\n",
        "          'created_utc',\n",
        "          'id',\n",
        "          'is_original_content',\n",
        "          'link_flair_text',\n",
        "          'locked',\n",
        "          'name',\n",
        "          'num_comments',\n",
        "          'over_18',\n",
        "          'permalink',\n",
        "          'selftext',\n",
        "          'spoiler',\n",
        "          'upvote_ratio')\n",
        "list_of_submissions = []\n",
        "\n",
        "# Parse each submission into a dictionary of the lised fields.\n",
        "for submission in all_posts:\n",
        "    full = vars(submission)\n",
        "    sub_dict = {field:full[field] for field in fields}\n",
        "    list_of_submissions.append(sub_dict)\n",
        "\n",
        "# Create a python dataframe of these submissions.\n",
        "collected_data = pd.DataFrame.from_records(list_of_submissions)\n",
        "\n",
        "# Display the dataframe.\n",
        "display(collected_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWFn6XlM8BkV"
      },
      "source": [
        "### Saving Reddit Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the collected data to parquet format\n",
        "SUBMISSION_PARQUET_PATH = './data/wallstreetbets-collection.parquet'\n",
        "\n",
        "# Create a pyarrow schema for the data types.\n",
        "submission_schema = pyarrow.schema([\n",
        "    ('title', pyarrow.string()),\n",
        "    ('created_utc', pyarrow.float64()),\n",
        "    ('id', pyarrow.string()),\n",
        "    ('is_original_content', pyarrow.bool_()),\n",
        "    ('link_flair_text', pyarrow.string()),\n",
        "    ('locked', pyarrow.bool_()),\n",
        "    ('name', pyarrow.string()),\n",
        "    ('num_comments', pyarrow.int64()),\n",
        "    ('over_18', pyarrow.bool_()),\n",
        "    ('permalink', pyarrow.string()),\n",
        "    ('selftext', pyarrow.string()),\n",
        "    ('spoiler', pyarrow.bool_()),\n",
        "    ('upvote_ratio', pyarrow.float64()),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "leoYaEHt8BkV"
      },
      "outputs": [],
      "source": [
        "# If the parqet does not exist, create it.\n",
        "if not os.path.exists(SUBMISSION_PARQUET_PATH):\n",
        "    collected_data.to_parquet(SUBMISSION_PARQUET_PATH, engine='pyarrow', schema=submission_schema)\n",
        "\n",
        "# If the data file already exist, merge new data with the existing one.\n",
        "else:\n",
        "    old_parquet = pd.read_parquet(SUBMISSION_PARQUET_PATH, engine='pyarrow', schema=submission_schema)\n",
        "    new_parquet = pd.concat([old_parquet, collected_data])\n",
        "    new_parquet = new_parquet.drop_duplicates(subset=['id','title','created_utc','name','permalink'], keep='last').reset_index(drop=True)\n",
        "    new_parquet.to_parquet(SUBMISSION_PARQUET_PATH, engine='pyarrow', schema=submission_schema)\n",
        "\n",
        "# Use the new collected data to get comment stuff.\n",
        "SUBMISSION_PARQUET_PATH = './data/wallstreetbets-collection.parquet'\n",
        "submission_collection = pd.read_parquet(SUBMISSION_PARQUET_PATH, engine='pyarrow', schema=submission_schema)\n",
        "display(submission_collection)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A52zTdg88BkV"
      },
      "source": [
        "## Part 3: Collecting Comments from Reddit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_K-voA38BkV"
      },
      "source": [
        "### Creating a database of reddit threads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXHu7mFu8BkV"
      },
      "outputs": [],
      "source": [
        "# Use the same methofology whih we used to collect submissions, but we'll add a parent submission id. and parent comment id.\n",
        "# Since the comment section can be very deep, we'll limit comments to a breadth of 10.\n",
        "# This may still be a lot more comments than we need for larger discussions.\n",
        "def extract_comments_from_submission(submission_id: str):\n",
        "    try:\n",
        "        submission = reddit.submission(id=submission_id)\n",
        "        submission.comments.replace_more(limit=10)  # Limit to 10 levels of comments\n",
        "        comments = []\n",
        "\n",
        "        for comment in submission.comments.list():\n",
        "            if isinstance(comment, praw.models.MoreComments):\n",
        "                continue\n",
        "\n",
        "            # NOTE: It looks like the top comment may be a user report. We'll ignore if is has certain text.\n",
        "            SKIPTEXT = '**User Report**'\n",
        "            if SKIPTEXT in comment.body:\n",
        "                continue\n",
        "\n",
        "            # Append the comment data to the list\n",
        "            comments.append({\n",
        "                'parent_post_id': submission_id,\n",
        "                'parent_comment_id': comment.parent_id,\n",
        "                'comment_id': comment.id,\n",
        "                'author': str(comment.author),\n",
        "                'created_utc': comment.created_utc,\n",
        "                'score': comment.score,\n",
        "                'body': comment.body\n",
        "            })\n",
        "\n",
        "        return comments\n",
        "\n",
        "    except Exception as e:\n",
        "        # Get the HTTP error code if available\n",
        "        if hasattr(e, 'response') and e.response is not None:\n",
        "            error_code = e.response.status_code\n",
        "            print(f\"HTTP Error {error_code} while fetching comments for submission {submission_id}\")\n",
        "        else:\n",
        "            error_code = None\n",
        "\n",
        "        # Print the an erroor message and return nothing.\n",
        "        print(f\"Error fetching comments for submission {submission_id}: {e}\")\n",
        "        return []\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYeljFKM8BkV"
      },
      "outputs": [],
      "source": [
        "# Show the results from one submission's comments\n",
        "submission_id = submission_collection.iloc[0]['id']\n",
        "\n",
        "# How many actual comments are there for this submission?\n",
        "submission = reddit.submission(id=submission_id)\n",
        "print(f\"Submission ID: {submission_id}\")\n",
        "print(f\"Title: {submission.title}\")\n",
        "print(f\"Number of comments: {submission.num_comments}\")\n",
        "\n",
        "# Get the comments for the submission\n",
        "results = extract_comments_from_submission(submission_id)\n",
        "\n",
        "# Create a dataframe of the comments\n",
        "comments_df = pd.DataFrame(results)\n",
        "\n",
        "# Display the comments dataframe\n",
        "display(comments_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vB3PNPj-8BkV"
      },
      "outputs": [],
      "source": [
        "# Collect the comments for all the submissions.\n",
        "all_comments = []\n",
        "for submission in tqdm(submission_collection['id'], desc=\"Fetching comments for all submissions\", unit=\"submission\"):    \n",
        "    comments = extract_comments_from_submission(submission)\n",
        "    all_comments.extend(comments)\n",
        "    # time.sleep(1)  # Optional delay to avoid rate limits\n",
        "\n",
        "# Create a python dataframe of these comments.\n",
        "comments_df = pd.DataFrame.from_records(all_comments)\n",
        "display(comments_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the collected data to parquet format\n",
        "COMMENT_PARQUET_PATH = './data/wallstreetbets-comment-collection.parquet'\n",
        "\n",
        "# Create a pyarrow schema for the comment data\n",
        "comment_schema = pyarrow.schema([\n",
        "    ('parent_post_id', pyarrow.string()),\n",
        "    ('parent_comment_id', pyarrow.string()),\n",
        "    ('comment_id', pyarrow.string()),\n",
        "    ('author', pyarrow.string()),\n",
        "    ('created_utc', pyarrow.float64()),\n",
        "    ('score', pyarrow.int64()),\n",
        "    ('body', pyarrow.string())\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNVWhQB_8BkW"
      },
      "outputs": [],
      "source": [
        "# Write the comments to parquet file. If it exists, append to it.\n",
        "if not os.path.exists(COMMENT_PARQUET_PATH):\n",
        "    comments_df.to_parquet(COMMENT_PARQUET_PATH, engine='pyarrow', schema=comment_schema)\n",
        "else:\n",
        "    old_parquet = pd.read_parquet(COMMENT_PARQUET_PATH, engine='pyarrow', schema=comment_schema)\n",
        "    new_parquet = pd.concat([old_parquet, comments_df])\n",
        "    new_parquet = new_parquet.drop_duplicates(subset=['parent_post_id','parent_comment_id','author','created_utc'], keep='last').reset_index(drop=True)\n",
        "    new_parquet.to_parquet(COMMENT_PARQUET_PATH, engine='pyarrow', schema=comment_schema)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 4: Collecting More Data (w/ Kaggle)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Read in the new reddit data from kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download path for the kaggle dataset.\n",
        "KAGGLE_DATASET_PATH = './data/kaggle-dataset'\n",
        "\n",
        "kaggle.api.authenticate()\n",
        "kaggle.api.dataset_download_files('gpreda/reddit-wallstreetsbets-posts', path='./data/', unzip=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Path to kaggle dataset.\n",
        "KAGGLE_DATASET_RAW_PATH = './data/reddit_wsb.csv'\n",
        "\n",
        "# Read the kaggle dataset.\n",
        "kaggle_df = pd.read_csv(KAGGLE_DATASET_RAW_PATH)\n",
        "\n",
        "# Drop the timestamp column if it exists.\n",
        "if 'timestamp' in kaggle_df.columns:\n",
        "    kaggle_df = kaggle_df.drop(columns=['timestamp'])\n",
        "\n",
        "# Enforce the schema.\n",
        "kaggle_df = kaggle_df.astype({\n",
        "    'title': 'string',\n",
        "    'score': 'int64',\n",
        "    'id': 'string',\n",
        "    'url': 'string',\n",
        "    'comms_num': 'int64',\n",
        "    'created': 'float64',\n",
        "    'body': 'string',\n",
        "})\n",
        "\n",
        "# Display the kaggle dataset.\n",
        "display(kaggle_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mapping to previous schema.\n",
        "kaggle_mapping = {\n",
        "    'title': 'title',\n",
        "    'score': 'upvote_ratio',\n",
        "    'id': 'id',\n",
        "    'url': 'permalink',\n",
        "    'comms_num': 'num_comments',\n",
        "    'created': 'created_utc',\n",
        "    'body': 'selftext'\n",
        "}\n",
        "\n",
        "# Rename the columns to match our schema.\n",
        "kaggle_df.rename(columns=kaggle_mapping, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Path to kaggle dataset final.\n",
        "KAGGLE_DATASET_FINAL_PATH = './data/kaggle-reddit-wsb.parquet'\n",
        "\n",
        "# Schema for the kaggle dataset.\n",
        "kaggle_schema = pyarrow.schema([\n",
        "    ('title', pyarrow.string()),\n",
        "    ('upvote_ratio', pyarrow.int64()),\n",
        "    ('id', pyarrow.string()),\n",
        "    ('permalink', pyarrow.string()),\n",
        "    ('num_comments', pyarrow.int64()),\n",
        "    ('created_utc', pyarrow.float64()),\n",
        "    ('selftext', pyarrow.string()),\n",
        "])\n",
        "\n",
        "# write the dataframe to parquet.\n",
        "kaggle_df.to_parquet(KAGGLE_DATASET_FINAL_PATH, engine='pyarrow', schema=kaggle_schema)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Merge the new data table with the old data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read in the datasets.\n",
        "praw_dataset = pd.read_parquet(SUBMISSION_PARQUET_PATH, engine='pyarrow', schema=submission_schema)\n",
        "kaggle_dataset = pd.read_parquet(KAGGLE_DATASET_FINAL_PATH, engine='pyarrow', schema=kaggle_schema)\n",
        "\n",
        "# Create similar columns with only the columns of the kaggle dataset.\n",
        "praw_dataset = praw_dataset[['title', 'upvote_ratio', 'id', 'permalink', 'num_comments', 'created_utc', 'selftext']]\n",
        "kaggle_dataset = kaggle_dataset[['title', 'upvote_ratio', 'id', 'permalink', 'num_comments', 'created_utc', 'selftext']]\n",
        "\n",
        "# Use the mapping to merge the two datasets.\n",
        "merged_dataset = pd.concat([praw_dataset, kaggle_dataset], ignore_index=True)\n",
        "\n",
        "# Remove duplicates based on the 'id' column.\n",
        "merged_dataset = merged_dataset.drop_duplicates(subset=['id'], keep='last').reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creare a final schema for the merged dataset.\n",
        "merged_schema = pyarrow.schema([\n",
        "    ('title', pyarrow.string()),\n",
        "    ('upvote_ratio', pyarrow.float64()),\n",
        "    ('id', pyarrow.string()),\n",
        "    ('permalink', pyarrow.string()),\n",
        "    ('num_comments', pyarrow.int64()),\n",
        "    ('created_utc', pyarrow.float64()),\n",
        "    ('selftext', pyarrow.string())\n",
        "])\n",
        "\n",
        "# Save the merged dataset to parquet format.\n",
        "MERGED_DATASET_PATH = './data/merged-reddit-wsb.parquet'\n",
        "merged_dataset.to_parquet(MERGED_DATASET_PATH, engine='pyarrow', schema=merged_schema)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display properties of the merged dataset.\n",
        "merged_dataset.info()\n",
        "print(\"-\" * 50)\n",
        "print(\"Merged dataset saved to:\", MERGED_DATASET_PATH)\n",
        "print(\"Number of columns in merged dataset:\", len(merged_dataset.columns))\n",
        "print(\"Columns in merged dataset:\", merged_dataset.columns.tolist())\n",
        "print(\"-\" * 50)\n",
        "print(\"Number of rows in praw dataset:\", len(praw_dataset))\n",
        "print(\"Number of rows in kaggle dataset:\", len(kaggle_dataset))\n",
        "print(\"Number of rows in merged dataset:\", len(merged_dataset))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
