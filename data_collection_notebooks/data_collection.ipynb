{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Collecting Reddit Data from r/WallStreetBets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mqH72oV8BkP"
      },
      "source": [
        "## Part 1: Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoSNYlmE8BkQ"
      },
      "source": [
        "### Install Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load environment variables from .env file\n",
        "from dotenv import load_dotenv, dotenv_values\n",
        "load_dotenv('.env')\n",
        "config = dotenv_values()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNHmHw-T8BkQ",
        "outputId": "5682a658-2c7f-4d24-e12c-866f8ade2b85"
      },
      "outputs": [],
      "source": [
        "# Import Packages\n",
        "import praw, time, os, pyarrow\n",
        "from IPython.display import display\n",
        "from requests import Session\n",
        "import pandas as pd\n",
        "from IPython import get_ipython\n",
        "from tqdm import tqdm \n",
        "import kagglehub, kaggle\n",
        "\n",
        "\n",
        "# Get config from colab or other environment.\n",
        "def is_colab():\n",
        "    return get_ipython().__class__.__module__ == \"google.colab._shell\"\n",
        "\n",
        "if is_colab():\n",
        "    from google.colab import userdata\n",
        "    config = {}\n",
        "    config['CLIENT_SECRET'] = userdata.get('CLIENT_SECRET')\n",
        "    config['CLIENT_ID'] = userdata.get('CLIENT_ID')\n",
        "    config['NAME'] = userdata.get('NAME')\n",
        "    config['REDIRECT_URI'] = userdata.get('REDIRECT_URI')\n",
        "    config['USERNAME'] = userdata.get('USERNAME')\n",
        "    config['PASSWORD'] = userdata.get('PASSWORD')\n",
        "\n",
        "else:\n",
        "    load_dotenv('.env')\n",
        "    config = dotenv_values()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQ6nQKwq8BkR"
      },
      "source": [
        "## Part 2: Collecting Submissions from Reddit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIqcjmZA8BkR"
      },
      "source": [
        "### Open Reddit Connection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "mkZtaceY8BkR",
        "outputId": "a1b8a895-f6f3-422f-dac8-b7794d675fe2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully logged in to Reddit!\n",
            "Logged in as: u/GregorybLafayetteML\n"
          ]
        }
      ],
      "source": [
        "# Create a custom session with a timeout\n",
        "session = Session()\n",
        "session.headers.update({'User-Agent': 'praw'})\n",
        "session.timeout = 10  # Set a timeout of 10 seconds\n",
        "\n",
        "# Login to Reddit using PRAW\n",
        "reddit = praw.Reddit(\n",
        "    client_id=config['CLIENT_ID'],\n",
        "    client_secret=config['CLIENT_SECRET'],\n",
        "    requestor_kwargs={\"session\": session},\n",
        "    username=config['USERNAME'],\n",
        "    password=config['PASSWORD'],\n",
        "    user_agent=\"CS470 ML Project Access by u/GregorybLafayetteML\"\n",
        ")\n",
        "\n",
        "# Add some peripheral config data\n",
        "reddit.config.log_requests = 1\n",
        "reddit.config.store_json_result = True\n",
        "\n",
        "# Test the connection\n",
        "try:\n",
        "    username = reddit.user.me()\n",
        "    print(\"Successfully logged in to Reddit!\")\n",
        "    print(f\"Logged in as: u/{username}\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to log in: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tk6FGRfK8BkS"
      },
      "source": [
        "### Accessing Reddit Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnIgfIks8BkT"
      },
      "source": [
        "To access reddit posts, we'll need send a request with the number of post we want to get. The following example finds the top 10 hottest posts on the u/wallstreetbets subreddit. We'll show the post title, score, flair, and URL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "MMebUrT98BkT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 10 hot posts from r/wallstreetbets:\n",
            "Title: Weekend Discussion Thread for the Weekend of May 09, 2025, Score: 135, Flair: Weekend Discussion, URL: https://www.reddit.com/r/wallstreetbets/comments/1kirpy3/weekend_discussion_thread_for_the_weekend_of_may/\n",
            "Title: Weekly Earnings Thread 5/12 - 5/16, Score: 106, Flair: Earnings Thread, URL: https://i.redd.it/4z7v8xc4srze1.jpeg\n",
            "Title: I turned $55 into $1544 from Microsoft earnings üåù, Score: 223, Flair: Gain, URL: https://i.redd.it/1w1dv0gi500f1.jpeg\n",
            "Title: US and China end trade talks for the night,  will continue talks on Sunday., Score: 149, Flair: Discussion, URL: https://www.reddit.com/r/wallstreetbets/comments/1kji1re/us_and_china_end_trade_talks_for_the_night_will/\n",
            "Title: Withdrew my 401k from my days in the army and put 2k on calls for. . . How screwed am I Monday?, Score: 127, Flair: YOLO, URL: https://www.reuters.com/world/us-china-trade-talks-geneva-end-night-continue-sunday-source-says-2025-05-10/\n",
            "Title: Green smoke coming out of the White House signifies the stock market is up today, Score: 24255, Flair: Meme, URL: https://i.redd.it/p0mn6t5u6tze1.jpeg\n",
            "Title: 1900 percent gain of 6.2k in 3 mins., Score: 145, Flair: Gain, URL: https://www.reddit.com/gallery/1kjfsf3\n",
            "Title: Thought RH enabling options in UK would make me rich, Score: 152, Flair: Loss, URL: https://www.reddit.com/gallery/1kj7ji8\n",
            "Title: PLTR YOLO Win, Score: 61, Flair: YOLO, URL: https://www.reddit.com/gallery/1kjb32p\n",
            "Title: Tesla tells Model Y and Cybertruck workers to stay home for a week, Score: 13, Flair: News, URL: https://www.businessinsider.com/tesla-model-y-cybertruck-workers-stay-home-memorial-day-2025-5?utm_source=reddit.com\n"
          ]
        }
      ],
      "source": [
        "top_posts = reddit.subreddit('wallstreetbets').hot(limit=10)\n",
        "print(\"Top 10 hot posts from r/wallstreetbets:\")\n",
        "for post in top_posts:\n",
        "    print(f\"Title: {post.title}, Score: {post.score}, Flair: {post.link_flair_text}, URL: {post.url}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IxVa5Xi8BkU"
      },
      "source": [
        "For this project, we'll need far more than ten posts at a time. The reddit API will limit our access to 100 posts at a time. Fortunately, the api uses a ListingGenerator which allows us to access our metered connection in sequential blocks. The following example shows how we can utilize this behavior, grabbing blocks of 100 posts at a time. In our example, we'll grab blocks of posts until we reach 5000 posts or our access times out. Notice that the procedure ends early with around 750-800 posts collected. The results are sparce, because our connection either timed out or was metered down by reddit. The latter option is more likely."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "WG2zyxFP8BkU"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Storing batch of posts: 50post [00:00, 61.17post/s]\n",
            "Storing batch of posts: 50post [00:00, 79.75post/s]\n",
            "Storing batch of posts: 50post [00:00, 67.59post/s]\n",
            "Storing batch of posts: 50post [00:00, 82.55post/s]\n",
            "Storing batch of posts: 50post [00:00, 76.38post/s]\n",
            "Storing batch of posts: 50post [00:00, 73.66post/s]\n",
            "Storing batch of posts: 50post [00:00, 75.55post/s]\n",
            "Storing batch of posts: 50post [00:00, 66.98post/s]\n",
            "Storing batch of posts: 50post [00:00, 77.49post/s]\n",
            "Storing batch of posts: 50post [00:00, 73.74post/s]\n",
            "Storing batch of posts: 50post [00:01, 27.38post/s]\n",
            "Storing batch of posts: 50post [00:00, 61.65post/s]\n",
            "Storing batch of posts: 50post [00:00, 66.31post/s]\n",
            "Storing batch of posts: 50post [00:00, 68.41post/s]\n",
            "Storing batch of posts: 50post [00:00, 56.21post/s]\n",
            "Storing batch of posts: 50post [00:00, 84.70post/s]\n",
            "Storing batch of posts: 50post [00:00, 67.74post/s]\n",
            "Storing batch of posts: 4post [00:00, 22.35post/s]\n",
            "Storing batch of posts: 0post [00:00, ?post/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No more posts to fetch.\n",
            "Fetched 854 posts in total.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Access the subreddit\n",
        "subreddit = reddit.subreddit(\"wallstreetbets\")\n",
        "\n",
        "# Initialize variables\n",
        "batch_size = 50 # Number of posts per batch\n",
        "total_posts = 5000  # Total number of posts to fetch\n",
        "all_posts = []  # To store all the retrieved posts\n",
        "after = None  # To keep track of the last post for pagination\n",
        "\n",
        "# Fetch posts in batches\n",
        "while len(all_posts) < total_posts:\n",
        "    # Fetch the next batch of posts\n",
        "    submissions = subreddit.new(limit=batch_size, params={\"after\": after})\n",
        "\n",
        "    batch_posts = []\n",
        "    for submission in tqdm(submissions, desc=\"Storing batch of posts\", unit=\"post\"):\n",
        "        batch_posts.append(submission)\n",
        "\n",
        "        # Update the `after` variable with the last submission's fullname\n",
        "        after = submission.fullname\n",
        "\n",
        "    # Add the batch to the main list\n",
        "    all_posts.extend(batch_posts)\n",
        "\n",
        "    # Exit loop if no more posts are available\n",
        "    if not batch_posts:\n",
        "        print(\"No more posts to fetch.\")\n",
        "        break\n",
        "\n",
        "    # Optional delay to avoid rate limits\n",
        "    time.sleep(1)  # Adjust the delay as necessary\n",
        "\n",
        "# Process the data (example: print the total number of posts fetched)\n",
        "print(f\"Fetched {len(all_posts)} posts in total.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_4N4b2q8BkU"
      },
      "source": [
        "Now that we have collected a large portion of posts/submssions, we'll parse the results and construct a dataframe with this data. We're going to collect more fields from this data than we might need right now, avoiding data limitations in the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "69crArfG8BkU"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>created_utc</th>\n",
              "      <th>id</th>\n",
              "      <th>is_original_content</th>\n",
              "      <th>link_flair_text</th>\n",
              "      <th>locked</th>\n",
              "      <th>name</th>\n",
              "      <th>num_comments</th>\n",
              "      <th>over_18</th>\n",
              "      <th>permalink</th>\n",
              "      <th>selftext</th>\n",
              "      <th>spoiler</th>\n",
              "      <th>upvote_ratio</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Tesla tells Model Y and Cybertruck workers to ...</td>\n",
              "      <td>1.746910e+09</td>\n",
              "      <td>1kjk032</td>\n",
              "      <td>False</td>\n",
              "      <td>News</td>\n",
              "      <td>False</td>\n",
              "      <td>t3_1kjk032</td>\n",
              "      <td>4</td>\n",
              "      <td>False</td>\n",
              "      <td>/r/wallstreetbets/comments/1kjk032/tesla_tells...</td>\n",
              "      <td>I can‚Äôt help but think this is a serious bull ...</td>\n",
              "      <td>False</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The key people in the USA-China meeting are no...</td>\n",
              "      <td>1.746910e+09</td>\n",
              "      <td>1kjjxyw</td>\n",
              "      <td>False</td>\n",
              "      <td>Discussion</td>\n",
              "      <td>False</td>\n",
              "      <td>t3_1kjjxyw</td>\n",
              "      <td>9</td>\n",
              "      <td>False</td>\n",
              "      <td>/r/wallstreetbets/comments/1kjjxyw/the_key_peo...</td>\n",
              "      <td>I noticed that we sent our head of finance Sco...</td>\n",
              "      <td>False</td>\n",
              "      <td>0.78</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Cut losses or hodl? Anyone else on the same boat</td>\n",
              "      <td>1.746909e+09</td>\n",
              "      <td>1kjjmgf</td>\n",
              "      <td>False</td>\n",
              "      <td>Loss</td>\n",
              "      <td>False</td>\n",
              "      <td>t3_1kjjmgf</td>\n",
              "      <td>8</td>\n",
              "      <td>False</td>\n",
              "      <td>/r/wallstreetbets/comments/1kjjmgf/cut_losses_...</td>\n",
              "      <td>Perfect timing buying $BULL calls before the o...</td>\n",
              "      <td>False</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Withdrew my 401k from my days in the army and ...</td>\n",
              "      <td>1.746907e+09</td>\n",
              "      <td>1kjitpt</td>\n",
              "      <td>False</td>\n",
              "      <td>YOLO</td>\n",
              "      <td>False</td>\n",
              "      <td>t3_1kjitpt</td>\n",
              "      <td>162</td>\n",
              "      <td>False</td>\n",
              "      <td>/r/wallstreetbets/comments/1kjitpt/withdrew_my...</td>\n",
              "      <td>I have 13 calls on uvxy between 32-40. I pulle...</td>\n",
              "      <td>False</td>\n",
              "      <td>0.78</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>US and China end trade talks for the night,  w...</td>\n",
              "      <td>1.746905e+09</td>\n",
              "      <td>1kji1re</td>\n",
              "      <td>False</td>\n",
              "      <td>Discussion</td>\n",
              "      <td>False</td>\n",
              "      <td>t3_1kji1re</td>\n",
              "      <td>87</td>\n",
              "      <td>False</td>\n",
              "      <td>/r/wallstreetbets/comments/1kji1re/us_and_chin...</td>\n",
              "      <td>Trade talks to continue tomorrow.\\n\\nhttps://w...</td>\n",
              "      <td>False</td>\n",
              "      <td>0.97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>849</th>\n",
              "      <td>TSMC first-quarter profit tops estimates, risi...</td>\n",
              "      <td>1.744869e+09</td>\n",
              "      <td>1k15iv3</td>\n",
              "      <td>False</td>\n",
              "      <td>News</td>\n",
              "      <td>False</td>\n",
              "      <td>t3_1k15iv3</td>\n",
              "      <td>39</td>\n",
              "      <td>False</td>\n",
              "      <td>/r/wallstreetbets/comments/1k15iv3/tsmc_firstq...</td>\n",
              "      <td></td>\n",
              "      <td>False</td>\n",
              "      <td>0.97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>850</th>\n",
              "      <td>Mortgage rates soar, prompting home buyers to ...</td>\n",
              "      <td>1.744867e+09</td>\n",
              "      <td>1k156sh</td>\n",
              "      <td>False</td>\n",
              "      <td>News</td>\n",
              "      <td>False</td>\n",
              "      <td>t3_1k156sh</td>\n",
              "      <td>351</td>\n",
              "      <td>False</td>\n",
              "      <td>/r/wallstreetbets/comments/1k156sh/mortgage_ra...</td>\n",
              "      <td>The Big Short Part 2: Electric Spoon</td>\n",
              "      <td>False</td>\n",
              "      <td>0.98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>851</th>\n",
              "      <td>Long calls. Am i safe?</td>\n",
              "      <td>1.744867e+09</td>\n",
              "      <td>1k152pc</td>\n",
              "      <td>False</td>\n",
              "      <td>Discussion</td>\n",
              "      <td>False</td>\n",
              "      <td>t3_1k152pc</td>\n",
              "      <td>53</td>\n",
              "      <td>False</td>\n",
              "      <td>/r/wallstreetbets/comments/1k152pc/long_calls_...</td>\n",
              "      <td>Long calls. Am i safe?</td>\n",
              "      <td>False</td>\n",
              "      <td>0.86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>852</th>\n",
              "      <td>DIS ber case.. more downside?</td>\n",
              "      <td>1.744867e+09</td>\n",
              "      <td>1k15220</td>\n",
              "      <td>False</td>\n",
              "      <td>Discussion</td>\n",
              "      <td>False</td>\n",
              "      <td>t3_1k15220</td>\n",
              "      <td>22</td>\n",
              "      <td>False</td>\n",
              "      <td>/r/wallstreetbets/comments/1k15220/dis_ber_cas...</td>\n",
              "      <td>Aside from the recent Snow White debacle, here...</td>\n",
              "      <td>False</td>\n",
              "      <td>0.56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>853</th>\n",
              "      <td>TSLA ‚Äî&gt; 3.75k gains on PUTs</td>\n",
              "      <td>1.744866e+09</td>\n",
              "      <td>1k14x1d</td>\n",
              "      <td>False</td>\n",
              "      <td>Gain</td>\n",
              "      <td>False</td>\n",
              "      <td>t3_1k14x1d</td>\n",
              "      <td>5</td>\n",
              "      <td>False</td>\n",
              "      <td>/r/wallstreetbets/comments/1k14x1d/tsla_375k_g...</td>\n",
              "      <td></td>\n",
              "      <td>False</td>\n",
              "      <td>0.80</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>854 rows √ó 13 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 title   created_utc       id  \\\n",
              "0    Tesla tells Model Y and Cybertruck workers to ...  1.746910e+09  1kjk032   \n",
              "1    The key people in the USA-China meeting are no...  1.746910e+09  1kjjxyw   \n",
              "2     Cut losses or hodl? Anyone else on the same boat  1.746909e+09  1kjjmgf   \n",
              "3    Withdrew my 401k from my days in the army and ...  1.746907e+09  1kjitpt   \n",
              "4    US and China end trade talks for the night,  w...  1.746905e+09  1kji1re   \n",
              "..                                                 ...           ...      ...   \n",
              "849  TSMC first-quarter profit tops estimates, risi...  1.744869e+09  1k15iv3   \n",
              "850  Mortgage rates soar, prompting home buyers to ...  1.744867e+09  1k156sh   \n",
              "851                             Long calls. Am i safe?  1.744867e+09  1k152pc   \n",
              "852                      DIS ber case.. more downside?  1.744867e+09  1k15220   \n",
              "853                        TSLA ‚Äî> 3.75k gains on PUTs  1.744866e+09  1k14x1d   \n",
              "\n",
              "     is_original_content link_flair_text  locked        name  num_comments  \\\n",
              "0                  False            News   False  t3_1kjk032             4   \n",
              "1                  False      Discussion   False  t3_1kjjxyw             9   \n",
              "2                  False            Loss   False  t3_1kjjmgf             8   \n",
              "3                  False            YOLO   False  t3_1kjitpt           162   \n",
              "4                  False      Discussion   False  t3_1kji1re            87   \n",
              "..                   ...             ...     ...         ...           ...   \n",
              "849                False            News   False  t3_1k15iv3            39   \n",
              "850                False            News   False  t3_1k156sh           351   \n",
              "851                False      Discussion   False  t3_1k152pc            53   \n",
              "852                False      Discussion   False  t3_1k15220            22   \n",
              "853                False            Gain   False  t3_1k14x1d             5   \n",
              "\n",
              "     over_18                                          permalink  \\\n",
              "0      False  /r/wallstreetbets/comments/1kjk032/tesla_tells...   \n",
              "1      False  /r/wallstreetbets/comments/1kjjxyw/the_key_peo...   \n",
              "2      False  /r/wallstreetbets/comments/1kjjmgf/cut_losses_...   \n",
              "3      False  /r/wallstreetbets/comments/1kjitpt/withdrew_my...   \n",
              "4      False  /r/wallstreetbets/comments/1kji1re/us_and_chin...   \n",
              "..       ...                                                ...   \n",
              "849    False  /r/wallstreetbets/comments/1k15iv3/tsmc_firstq...   \n",
              "850    False  /r/wallstreetbets/comments/1k156sh/mortgage_ra...   \n",
              "851    False  /r/wallstreetbets/comments/1k152pc/long_calls_...   \n",
              "852    False  /r/wallstreetbets/comments/1k15220/dis_ber_cas...   \n",
              "853    False  /r/wallstreetbets/comments/1k14x1d/tsla_375k_g...   \n",
              "\n",
              "                                              selftext  spoiler  upvote_ratio  \n",
              "0    I can‚Äôt help but think this is a serious bull ...    False          1.00  \n",
              "1    I noticed that we sent our head of finance Sco...    False          0.78  \n",
              "2    Perfect timing buying $BULL calls before the o...    False          1.00  \n",
              "3    I have 13 calls on uvxy between 32-40. I pulle...    False          0.78  \n",
              "4    Trade talks to continue tomorrow.\\n\\nhttps://w...    False          0.97  \n",
              "..                                                 ...      ...           ...  \n",
              "849                                                       False          0.97  \n",
              "850               The Big Short Part 2: Electric Spoon    False          0.98  \n",
              "851                            Long calls. Am i safe?     False          0.86  \n",
              "852  Aside from the recent Snow White debacle, here...    False          0.56  \n",
              "853                                                       False          0.80  \n",
              "\n",
              "[854 rows x 13 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Parse are submission objects that we collected.\n",
        "fields = ('title',\n",
        "          'created_utc',\n",
        "          'id',\n",
        "          'is_original_content',\n",
        "          'link_flair_text',\n",
        "          'locked',\n",
        "          'name',\n",
        "          'num_comments',\n",
        "          'over_18',\n",
        "          'permalink',\n",
        "          'selftext',\n",
        "          'spoiler',\n",
        "          'upvote_ratio')\n",
        "list_of_submissions = []\n",
        "\n",
        "# Parse each submission into a dictionary of the lised fields.\n",
        "for submission in all_posts:\n",
        "    full = vars(submission)\n",
        "    sub_dict = {field:full[field] for field in fields}\n",
        "    list_of_submissions.append(sub_dict)\n",
        "\n",
        "# Create a python dataframe of these submissions.\n",
        "collected_data = pd.DataFrame.from_records(list_of_submissions)\n",
        "\n",
        "# Display the dataframe.\n",
        "display(collected_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWFn6XlM8BkV"
      },
      "source": [
        "### Saving Reddit Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the collected data to parquet format\n",
        "SUBMISSION_PARQUET_PATH = '../data/wallstreetbets-collection.parquet'\n",
        "\n",
        "# Create a pyarrow schema for the data types.\n",
        "submission_schema = pyarrow.schema([\n",
        "    ('title', pyarrow.string()),\n",
        "    ('created_utc', pyarrow.float64()),\n",
        "    ('id', pyarrow.string()),\n",
        "    ('is_original_content', pyarrow.bool_()),\n",
        "    ('link_flair_text', pyarrow.string()),\n",
        "    ('locked', pyarrow.bool_()),\n",
        "    ('name', pyarrow.string()),\n",
        "    ('num_comments', pyarrow.int64()),\n",
        "    ('over_18', pyarrow.bool_()),\n",
        "    ('permalink', pyarrow.string()),\n",
        "    ('selftext', pyarrow.string()),\n",
        "    ('spoiler', pyarrow.bool_()),\n",
        "    ('upvote_ratio', pyarrow.float64()),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "leoYaEHt8BkV"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>created_utc</th>\n",
              "      <th>id</th>\n",
              "      <th>is_original_content</th>\n",
              "      <th>link_flair_text</th>\n",
              "      <th>locked</th>\n",
              "      <th>name</th>\n",
              "      <th>num_comments</th>\n",
              "      <th>over_18</th>\n",
              "      <th>permalink</th>\n",
              "      <th>selftext</th>\n",
              "      <th>spoiler</th>\n",
              "      <th>upvote_ratio</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Nivea Along</td>\n",
              "      <td>1.744832e+09</td>\n",
              "      <td>1k0t4jk</td>\n",
              "      <td>False</td>\n",
              "      <td>YOLO</td>\n",
              "      <td>False</td>\n",
              "      <td>t3_1k0t4jk</td>\n",
              "      <td>5</td>\n",
              "      <td>False</td>\n",
              "      <td>/r/wallstreetbets/comments/1k0t4jk/nivea_along/</td>\n",
              "      <td>After -7% yesterday and -10% today</td>\n",
              "      <td>False</td>\n",
              "      <td>0.67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Powell to Volatile Stock Market: You‚Äôre on You...</td>\n",
              "      <td>1.744836e+09</td>\n",
              "      <td>1k0unbq</td>\n",
              "      <td>False</td>\n",
              "      <td>News</td>\n",
              "      <td>False</td>\n",
              "      <td>t3_1k0unbq</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>/r/wallstreetbets/comments/1k0unbq/powell_to_v...</td>\n",
              "      <td></td>\n",
              "      <td>False</td>\n",
              "      <td>0.86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Made back the last Wendy‚Äôs paycheck I lost</td>\n",
              "      <td>1.744834e+09</td>\n",
              "      <td>1k0tv2y</td>\n",
              "      <td>False</td>\n",
              "      <td>Gain</td>\n",
              "      <td>False</td>\n",
              "      <td>t3_1k0tv2y</td>\n",
              "      <td>6</td>\n",
              "      <td>False</td>\n",
              "      <td>/r/wallstreetbets/comments/1k0tv2y/made_back_t...</td>\n",
              "      <td></td>\n",
              "      <td>False</td>\n",
              "      <td>0.94</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>After market observation. When I finished buyi...</td>\n",
              "      <td>1.744833e+09</td>\n",
              "      <td>1k0tnqx</td>\n",
              "      <td>False</td>\n",
              "      <td>Gain</td>\n",
              "      <td>False</td>\n",
              "      <td>t3_1k0tnqx</td>\n",
              "      <td>8</td>\n",
              "      <td>False</td>\n",
              "      <td>/r/wallstreetbets/comments/1k0tnqx/after_marke...</td>\n",
              "      <td>https://preview.redd.it/41ilvj6f39ve1.png?widt...</td>\n",
              "      <td>False</td>\n",
              "      <td>0.72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Ominous</td>\n",
              "      <td>1.744833e+09</td>\n",
              "      <td>1k0thnd</td>\n",
              "      <td>False</td>\n",
              "      <td>Discussion</td>\n",
              "      <td>False</td>\n",
              "      <td>t3_1k0thnd</td>\n",
              "      <td>110</td>\n",
              "      <td>False</td>\n",
              "      <td>/r/wallstreetbets/comments/1k0thnd/ominous/</td>\n",
              "      <td>NVIDIA 2024 is starting to rhyme like Cisco 20...</td>\n",
              "      <td>False</td>\n",
              "      <td>0.85</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1709</th>\n",
              "      <td>TSMC first-quarter profit tops estimates, risi...</td>\n",
              "      <td>1.744869e+09</td>\n",
              "      <td>1k15iv3</td>\n",
              "      <td>False</td>\n",
              "      <td>News</td>\n",
              "      <td>False</td>\n",
              "      <td>t3_1k15iv3</td>\n",
              "      <td>39</td>\n",
              "      <td>False</td>\n",
              "      <td>/r/wallstreetbets/comments/1k15iv3/tsmc_firstq...</td>\n",
              "      <td></td>\n",
              "      <td>False</td>\n",
              "      <td>0.97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1710</th>\n",
              "      <td>Mortgage rates soar, prompting home buyers to ...</td>\n",
              "      <td>1.744867e+09</td>\n",
              "      <td>1k156sh</td>\n",
              "      <td>False</td>\n",
              "      <td>News</td>\n",
              "      <td>False</td>\n",
              "      <td>t3_1k156sh</td>\n",
              "      <td>351</td>\n",
              "      <td>False</td>\n",
              "      <td>/r/wallstreetbets/comments/1k156sh/mortgage_ra...</td>\n",
              "      <td>The Big Short Part 2: Electric Spoon</td>\n",
              "      <td>False</td>\n",
              "      <td>0.98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1711</th>\n",
              "      <td>Long calls. Am i safe?</td>\n",
              "      <td>1.744867e+09</td>\n",
              "      <td>1k152pc</td>\n",
              "      <td>False</td>\n",
              "      <td>Discussion</td>\n",
              "      <td>False</td>\n",
              "      <td>t3_1k152pc</td>\n",
              "      <td>53</td>\n",
              "      <td>False</td>\n",
              "      <td>/r/wallstreetbets/comments/1k152pc/long_calls_...</td>\n",
              "      <td>Long calls. Am i safe?</td>\n",
              "      <td>False</td>\n",
              "      <td>0.86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1712</th>\n",
              "      <td>DIS ber case.. more downside?</td>\n",
              "      <td>1.744867e+09</td>\n",
              "      <td>1k15220</td>\n",
              "      <td>False</td>\n",
              "      <td>Discussion</td>\n",
              "      <td>False</td>\n",
              "      <td>t3_1k15220</td>\n",
              "      <td>22</td>\n",
              "      <td>False</td>\n",
              "      <td>/r/wallstreetbets/comments/1k15220/dis_ber_cas...</td>\n",
              "      <td>Aside from the recent Snow White debacle, here...</td>\n",
              "      <td>False</td>\n",
              "      <td>0.56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1713</th>\n",
              "      <td>TSLA ‚Äî&gt; 3.75k gains on PUTs</td>\n",
              "      <td>1.744866e+09</td>\n",
              "      <td>1k14x1d</td>\n",
              "      <td>False</td>\n",
              "      <td>Gain</td>\n",
              "      <td>False</td>\n",
              "      <td>t3_1k14x1d</td>\n",
              "      <td>5</td>\n",
              "      <td>False</td>\n",
              "      <td>/r/wallstreetbets/comments/1k14x1d/tsla_375k_g...</td>\n",
              "      <td></td>\n",
              "      <td>False</td>\n",
              "      <td>0.80</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1714 rows √ó 13 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  title   created_utc  \\\n",
              "0                                           Nivea Along  1.744832e+09   \n",
              "1     Powell to Volatile Stock Market: You‚Äôre on You...  1.744836e+09   \n",
              "2            Made back the last Wendy‚Äôs paycheck I lost  1.744834e+09   \n",
              "3     After market observation. When I finished buyi...  1.744833e+09   \n",
              "4                                               Ominous  1.744833e+09   \n",
              "...                                                 ...           ...   \n",
              "1709  TSMC first-quarter profit tops estimates, risi...  1.744869e+09   \n",
              "1710  Mortgage rates soar, prompting home buyers to ...  1.744867e+09   \n",
              "1711                             Long calls. Am i safe?  1.744867e+09   \n",
              "1712                      DIS ber case.. more downside?  1.744867e+09   \n",
              "1713                        TSLA ‚Äî> 3.75k gains on PUTs  1.744866e+09   \n",
              "\n",
              "           id  is_original_content link_flair_text  locked        name  \\\n",
              "0     1k0t4jk                False            YOLO   False  t3_1k0t4jk   \n",
              "1     1k0unbq                False            News   False  t3_1k0unbq   \n",
              "2     1k0tv2y                False            Gain   False  t3_1k0tv2y   \n",
              "3     1k0tnqx                False            Gain   False  t3_1k0tnqx   \n",
              "4     1k0thnd                False      Discussion   False  t3_1k0thnd   \n",
              "...       ...                  ...             ...     ...         ...   \n",
              "1709  1k15iv3                False            News   False  t3_1k15iv3   \n",
              "1710  1k156sh                False            News   False  t3_1k156sh   \n",
              "1711  1k152pc                False      Discussion   False  t3_1k152pc   \n",
              "1712  1k15220                False      Discussion   False  t3_1k15220   \n",
              "1713  1k14x1d                False            Gain   False  t3_1k14x1d   \n",
              "\n",
              "      num_comments  over_18  \\\n",
              "0                5    False   \n",
              "1                2    False   \n",
              "2                6    False   \n",
              "3                8    False   \n",
              "4              110    False   \n",
              "...            ...      ...   \n",
              "1709            39    False   \n",
              "1710           351    False   \n",
              "1711            53    False   \n",
              "1712            22    False   \n",
              "1713             5    False   \n",
              "\n",
              "                                              permalink  \\\n",
              "0       /r/wallstreetbets/comments/1k0t4jk/nivea_along/   \n",
              "1     /r/wallstreetbets/comments/1k0unbq/powell_to_v...   \n",
              "2     /r/wallstreetbets/comments/1k0tv2y/made_back_t...   \n",
              "3     /r/wallstreetbets/comments/1k0tnqx/after_marke...   \n",
              "4           /r/wallstreetbets/comments/1k0thnd/ominous/   \n",
              "...                                                 ...   \n",
              "1709  /r/wallstreetbets/comments/1k15iv3/tsmc_firstq...   \n",
              "1710  /r/wallstreetbets/comments/1k156sh/mortgage_ra...   \n",
              "1711  /r/wallstreetbets/comments/1k152pc/long_calls_...   \n",
              "1712  /r/wallstreetbets/comments/1k15220/dis_ber_cas...   \n",
              "1713  /r/wallstreetbets/comments/1k14x1d/tsla_375k_g...   \n",
              "\n",
              "                                               selftext  spoiler  upvote_ratio  \n",
              "0                   After -7% yesterday and -10% today     False          0.67  \n",
              "1                                                          False          0.86  \n",
              "2                                                          False          0.94  \n",
              "3     https://preview.redd.it/41ilvj6f39ve1.png?widt...    False          0.72  \n",
              "4     NVIDIA 2024 is starting to rhyme like Cisco 20...    False          0.85  \n",
              "...                                                 ...      ...           ...  \n",
              "1709                                                       False          0.97  \n",
              "1710               The Big Short Part 2: Electric Spoon    False          0.98  \n",
              "1711                            Long calls. Am i safe?     False          0.86  \n",
              "1712  Aside from the recent Snow White debacle, here...    False          0.56  \n",
              "1713                                                       False          0.80  \n",
              "\n",
              "[1714 rows x 13 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# If the parqet does not exist, create it.\n",
        "if not os.path.exists(SUBMISSION_PARQUET_PATH):\n",
        "    collected_data.to_parquet(SUBMISSION_PARQUET_PATH, engine='pyarrow', schema=submission_schema)\n",
        "\n",
        "# If the data file already exist, merge new data with the existing one.\n",
        "else:\n",
        "    old_parquet = pd.read_parquet(SUBMISSION_PARQUET_PATH, engine='pyarrow', schema=submission_schema)\n",
        "    new_parquet = pd.concat([old_parquet, collected_data])\n",
        "    new_parquet = new_parquet.drop_duplicates(subset=['id','title','created_utc','name','permalink'], keep='last').reset_index(drop=True)\n",
        "    new_parquet.to_parquet(SUBMISSION_PARQUET_PATH, engine='pyarrow', schema=submission_schema)\n",
        "\n",
        "# Use the new collected data to get comment stuff.\n",
        "SUBMISSION_PARQUET_PATH = '../data/wallstreetbets-collection.parquet'\n",
        "submission_collection = pd.read_parquet(SUBMISSION_PARQUET_PATH, engine='pyarrow', schema=submission_schema)\n",
        "display(submission_collection)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A52zTdg88BkV"
      },
      "source": [
        "## Part 3: Collecting Comments from Reddit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_K-voA38BkV"
      },
      "source": [
        "### Creating a database of reddit threads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "XXHu7mFu8BkV"
      },
      "outputs": [],
      "source": [
        "# Use the same methofology whih we used to collect submissions, but we'll add a parent submission id. and parent comment id.\n",
        "# Since the comment section can be very deep, we'll limit comments to a breadth of 10.\n",
        "# This may still be a lot more comments than we need for larger discussions.\n",
        "def extract_comments_from_submission(submission_id: str):\n",
        "    try:\n",
        "        submission = reddit.submission(id=submission_id)\n",
        "        submission.comments.replace_more(limit=10)  # Limit to 10 levels of comments\n",
        "        comments = []\n",
        "\n",
        "        for comment in submission.comments.list():\n",
        "            if isinstance(comment, praw.models.MoreComments):\n",
        "                continue\n",
        "\n",
        "            # NOTE: It looks like the top comment may be a user report. We'll ignore if is has certain text.\n",
        "            SKIPTEXT = '**User Report**'\n",
        "            if SKIPTEXT in comment.body:\n",
        "                continue\n",
        "\n",
        "            # Append the comment data to the list\n",
        "            comments.append({\n",
        "                'parent_post_id': submission_id,\n",
        "                'parent_comment_id': comment.parent_id,\n",
        "                'comment_id': comment.id,\n",
        "                'author': str(comment.author),\n",
        "                'created_utc': comment.created_utc,\n",
        "                'score': comment.score,\n",
        "                'body': comment.body\n",
        "            })\n",
        "\n",
        "        return comments\n",
        "\n",
        "    except Exception as e:\n",
        "        # Get the HTTP error code if available\n",
        "        if hasattr(e, 'response') and e.response is not None:\n",
        "            error_code = e.response.status_code\n",
        "            print(f\"HTTP Error {error_code} while fetching comments for submission {submission_id}\")\n",
        "        else:\n",
        "            error_code = None\n",
        "\n",
        "        # Print the an erroor message and return nothing.\n",
        "        print(f\"Error fetching comments for submission {submission_id}: {e}\")\n",
        "        return []\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "gYeljFKM8BkV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Submission ID: 1k0t4jk\n",
            "Title: Nivea Along\n",
            "Number of comments: 6\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>parent_post_id</th>\n",
              "      <th>parent_comment_id</th>\n",
              "      <th>comment_id</th>\n",
              "      <th>author</th>\n",
              "      <th>created_utc</th>\n",
              "      <th>score</th>\n",
              "      <th>body</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1k0t4jk</td>\n",
              "      <td>t3_1k0t4jk</td>\n",
              "      <td>mngmxdi</td>\n",
              "      <td>chrissurra</td>\n",
              "      <td>1.744832e+09</td>\n",
              "      <td>5</td>\n",
              "      <td>You mean Neveah</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1k0t4jk</td>\n",
              "      <td>t3_1k0t4jk</td>\n",
              "      <td>mngn956</td>\n",
              "      <td>Alert_Barber_3105</td>\n",
              "      <td>1.744832e+09</td>\n",
              "      <td>6</td>\n",
              "      <td>The skin cream?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1k0t4jk</td>\n",
              "      <td>t3_1k0t4jk</td>\n",
              "      <td>mngoka2</td>\n",
              "      <td>Reasonable_Roger</td>\n",
              "      <td>1.744832e+09</td>\n",
              "      <td>1</td>\n",
              "      <td>Psoriasis calls</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1k0t4jk</td>\n",
              "      <td>t1_mngmxdi</td>\n",
              "      <td>mngnbpk</td>\n",
              "      <td>Own-Foundation3873</td>\n",
              "      <td>1.744832e+09</td>\n",
              "      <td>1</td>\n",
              "      <td>yesss sir</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1k0t4jk</td>\n",
              "      <td>t1_mngn956</td>\n",
              "      <td>mngnj1n</td>\n",
              "      <td>Own-Foundation3873</td>\n",
              "      <td>1.744832e+09</td>\n",
              "      <td>1</td>\n",
              "      <td>pow till it creams</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  parent_post_id parent_comment_id comment_id              author  \\\n",
              "0        1k0t4jk        t3_1k0t4jk    mngmxdi          chrissurra   \n",
              "1        1k0t4jk        t3_1k0t4jk    mngn956   Alert_Barber_3105   \n",
              "2        1k0t4jk        t3_1k0t4jk    mngoka2    Reasonable_Roger   \n",
              "3        1k0t4jk        t1_mngmxdi    mngnbpk  Own-Foundation3873   \n",
              "4        1k0t4jk        t1_mngn956    mngnj1n  Own-Foundation3873   \n",
              "\n",
              "    created_utc  score                body  \n",
              "0  1.744832e+09      5     You mean Neveah  \n",
              "1  1.744832e+09      6     The skin cream?  \n",
              "2  1.744832e+09      1     Psoriasis calls  \n",
              "3  1.744832e+09      1           yesss sir  \n",
              "4  1.744832e+09      1  pow till it creams  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Show the results from one submission's comments\n",
        "submission_id = submission_collection.iloc[0]['id']\n",
        "\n",
        "# How many actual comments are there for this submission?\n",
        "submission = reddit.submission(id=submission_id)\n",
        "print(f\"Submission ID: {submission_id}\")\n",
        "print(f\"Title: {submission.title}\")\n",
        "print(f\"Number of comments: {submission.num_comments}\")\n",
        "\n",
        "# Get the comments for the submission\n",
        "results = extract_comments_from_submission(submission_id)\n",
        "\n",
        "# Create a dataframe of the comments\n",
        "comments_df = pd.DataFrame(results)\n",
        "\n",
        "# Display the comments dataframe\n",
        "display(comments_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vB3PNPj-8BkV"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching comments for all submissions:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1015/1714 [25:38<32:52,  2.82s/submission] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HTTP Error 429 while fetching comments for submission 1kghdqf\n",
            "Error fetching comments for submission 1kghdqf: received 429 HTTP response\n",
            "HTTP Error 429 while fetching comments for submission 1kgggi0\n",
            "Error fetching comments for submission 1kgggi0: received 429 HTTP response\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching comments for all submissions:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 1241/1714 [30:29<29:32,  3.75s/submission]"
          ]
        }
      ],
      "source": [
        "# Collect the comments for all the submissions.\n",
        "all_comments = []\n",
        "for submission in tqdm(submission_collection['id'], desc=\"Fetching comments for all submissions\", unit=\"submission\"):    \n",
        "    comments = extract_comments_from_submission(submission)\n",
        "    all_comments.extend(comments)\n",
        "    # time.sleep(1)  # Optional delay to avoid rate limits\n",
        "\n",
        "# Create a python dataframe of these comments.\n",
        "comments_df = pd.DataFrame.from_records(all_comments)\n",
        "display(comments_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the collected data to parquet format\n",
        "COMMENT_PARQUET_PATH = '../data/wallstreetbets-comment-collection.parquet'\n",
        "\n",
        "# Create a pyarrow schema for the comment data\n",
        "comment_schema = pyarrow.schema([\n",
        "    ('parent_post_id', pyarrow.string()),\n",
        "    ('parent_comment_id', pyarrow.string()),\n",
        "    ('comment_id', pyarrow.string()),\n",
        "    ('author', pyarrow.string()),\n",
        "    ('created_utc', pyarrow.float64()),\n",
        "    ('score', pyarrow.int64()),\n",
        "    ('body', pyarrow.string())\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNVWhQB_8BkW"
      },
      "outputs": [],
      "source": [
        "# Write the comments to parquet file. If it exists, append to it.\n",
        "if not os.path.exists(COMMENT_PARQUET_PATH):\n",
        "    comments_df.to_parquet(COMMENT_PARQUET_PATH, engine='pyarrow', schema=comment_schema)\n",
        "else:\n",
        "    old_parquet = pd.read_parquet(COMMENT_PARQUET_PATH, engine='pyarrow', schema=comment_schema)\n",
        "    new_parquet = pd.concat([old_parquet, comments_df])\n",
        "    new_parquet = new_parquet.drop_duplicates(subset=['parent_post_id','parent_comment_id','author','created_utc'], keep='last').reset_index(drop=True)\n",
        "    new_parquet.to_parquet(COMMENT_PARQUET_PATH, engine='pyarrow', schema=comment_schema)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 4: Collecting More Data (w/ Kaggle)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Read in the new reddit data from kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download path for the kaggle dataset.\n",
        "KAGGLE_DATASET_PATH = '../data/kaggle-dataset'\n",
        "\n",
        "kaggle.api.authenticate()\n",
        "kaggle.api.dataset_download_files('gpreda/reddit-wallstreetsbets-posts', path='./data/', unzip=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Path to kaggle dataset.\n",
        "KAGGLE_DATASET_RAW_PATH = '../data/reddit_wsb.csv'\n",
        "\n",
        "# Read the kaggle dataset.\n",
        "kaggle_df = pd.read_csv(KAGGLE_DATASET_RAW_PATH)\n",
        "\n",
        "# Drop the timestamp column if it exists.\n",
        "if 'timestamp' in kaggle_df.columns:\n",
        "    kaggle_df = kaggle_df.drop(columns=['timestamp'])\n",
        "\n",
        "# Enforce the schema.\n",
        "kaggle_df = kaggle_df.astype({\n",
        "    'title': 'string',\n",
        "    'score': 'int64',\n",
        "    'id': 'string',\n",
        "    'url': 'string',\n",
        "    'comms_num': 'int64',\n",
        "    'created': 'float64',\n",
        "    'body': 'string',\n",
        "})\n",
        "\n",
        "# Display the kaggle dataset.\n",
        "display(kaggle_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mapping to previous schema.\n",
        "kaggle_mapping = {\n",
        "    'title': 'title',\n",
        "    'score': 'upvote_ratio',\n",
        "    'id': 'id',\n",
        "    'url': 'permalink',\n",
        "    'comms_num': 'num_comments',\n",
        "    'created': 'created_utc',\n",
        "    'body': 'selftext'\n",
        "}\n",
        "\n",
        "# Rename the columns to match our schema.\n",
        "kaggle_df.rename(columns=kaggle_mapping, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Path to kaggle dataset final.\n",
        "KAGGLE_DATASET_FINAL_PATH = '../data/kaggle-reddit-wsb.parquet'\n",
        "\n",
        "# Schema for the kaggle dataset.\n",
        "kaggle_schema = pyarrow.schema([\n",
        "    ('title', pyarrow.string()),\n",
        "    ('upvote_ratio', pyarrow.int64()),\n",
        "    ('id', pyarrow.string()),\n",
        "    ('permalink', pyarrow.string()),\n",
        "    ('num_comments', pyarrow.int64()),\n",
        "    ('created_utc', pyarrow.float64()),\n",
        "    ('selftext', pyarrow.string()),\n",
        "])\n",
        "\n",
        "# write the dataframe to parquet.\n",
        "kaggle_df.to_parquet(KAGGLE_DATASET_FINAL_PATH, engine='pyarrow', schema=kaggle_schema)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Merge the new data table with the old data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read in the datasets.\n",
        "praw_dataset = pd.read_parquet(SUBMISSION_PARQUET_PATH, engine='pyarrow', schema=submission_schema)\n",
        "kaggle_dataset = pd.read_parquet(KAGGLE_DATASET_FINAL_PATH, engine='pyarrow', schema=kaggle_schema)\n",
        "\n",
        "# Create similar columns with only the columns of the kaggle dataset.\n",
        "praw_dataset = praw_dataset[['title', 'upvote_ratio', 'id', 'permalink', 'num_comments', 'created_utc', 'selftext']]\n",
        "kaggle_dataset = kaggle_dataset[['title', 'upvote_ratio', 'id', 'permalink', 'num_comments', 'created_utc', 'selftext']]\n",
        "\n",
        "# Use the mapping to merge the two datasets.\n",
        "merged_dataset = pd.concat([praw_dataset, kaggle_dataset], ignore_index=True)\n",
        "\n",
        "# Remove duplicates based on the 'id' column.\n",
        "merged_dataset = merged_dataset.drop_duplicates(subset=['id'], keep='last').reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creare a final schema for the merged dataset.\n",
        "merged_schema = pyarrow.schema([\n",
        "    ('title', pyarrow.string()),\n",
        "    ('upvote_ratio', pyarrow.float64()),\n",
        "    ('id', pyarrow.string()),\n",
        "    ('permalink', pyarrow.string()),\n",
        "    ('num_comments', pyarrow.int64()),\n",
        "    ('created_utc', pyarrow.float64()),\n",
        "    ('selftext', pyarrow.string())\n",
        "])\n",
        "\n",
        "# Save the merged dataset to parquet format.\n",
        "MERGED_DATASET_PATH = '../data/merged-reddit-wsb.parquet'\n",
        "merged_dataset.to_parquet(MERGED_DATASET_PATH, engine='pyarrow', schema=merged_schema)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display properties of the merged dataset.\n",
        "merged_dataset.info()\n",
        "print(\"-\" * 50)\n",
        "print(\"Merged dataset saved to:\", MERGED_DATASET_PATH)\n",
        "print(\"Number of columns in merged dataset:\", len(merged_dataset.columns))\n",
        "print(\"Columns in merged dataset:\", merged_dataset.columns.tolist())\n",
        "print(\"-\" * 50)\n",
        "print(\"Number of rows in praw dataset:\", len(praw_dataset))\n",
        "print(\"Number of rows in kaggle dataset:\", len(kaggle_dataset))\n",
        "print(\"Number of rows in merged dataset:\", len(merged_dataset))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
