{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Collecting Reddit Data from r/WallStreetBets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mqH72oV8BkP"
      },
      "source": [
        "## Part 1: Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoSNYlmE8BkQ"
      },
      "source": [
        "### Install Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load environment variables from .env file\n",
        "from dotenv import load_dotenv, dotenv_values\n",
        "load_dotenv('.env')\n",
        "config = dotenv_values()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNHmHw-T8BkQ",
        "outputId": "5682a658-2c7f-4d24-e12c-866f8ade2b85"
      },
      "outputs": [],
      "source": [
        "# Import Packages\n",
        "import praw, time, os, pyarrow\n",
        "from IPython.display import display\n",
        "from requests import Session\n",
        "import pandas as pd\n",
        "from IPython import get_ipython\n",
        "from tqdm import tqdm \n",
        "import kagglehub, kaggle\n",
        "\n",
        "\n",
        "# Get config from colab or other environment.\n",
        "def is_colab():\n",
        "    return get_ipython().__class__.__module__ == \"google.colab._shell\"\n",
        "\n",
        "if is_colab():\n",
        "    from google.colab import userdata\n",
        "    config = {}\n",
        "    config['CLIENT_SECRET'] = userdata.get('CLIENT_SECRET')\n",
        "    config['CLIENT_ID'] = userdata.get('CLIENT_ID')\n",
        "    config['NAME'] = userdata.get('NAME')\n",
        "    config['REDIRECT_URI'] = userdata.get('REDIRECT_URI')\n",
        "    config['USERNAME'] = userdata.get('USERNAME')\n",
        "    config['PASSWORD'] = userdata.get('PASSWORD')\n",
        "\n",
        "else:\n",
        "    load_dotenv('.env')\n",
        "    config = dotenv_values()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQ6nQKwq8BkR"
      },
      "source": [
        "## Part 2: Collecting Submissions from Reddit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIqcjmZA8BkR"
      },
      "source": [
        "### Open Reddit Connection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "mkZtaceY8BkR",
        "outputId": "a1b8a895-f6f3-422f-dac8-b7794d675fe2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully logged in to Reddit!\n",
            "Logged in as: u/GregorybLafayetteML\n"
          ]
        }
      ],
      "source": [
        "# Create a custom session with a timeout\n",
        "session = Session()\n",
        "session.headers.update({'User-Agent': 'praw'})\n",
        "session.timeout = 10  # Set a timeout of 10 seconds\n",
        "\n",
        "# Login to Reddit using PRAW\n",
        "reddit = praw.Reddit(\n",
        "    client_id=config['CLIENT_ID'],\n",
        "    client_secret=config['CLIENT_SECRET'],\n",
        "    requestor_kwargs={\"session\": session},\n",
        "    username=config['USERNAME'],\n",
        "    password=config['PASSWORD'],\n",
        "    user_agent=\"CS470 ML Project Access by u/GregorybLafayetteML\"\n",
        ")\n",
        "\n",
        "# Add some peripheral config data\n",
        "reddit.config.log_requests = 1\n",
        "reddit.config.store_json_result = True\n",
        "\n",
        "# Test the connection\n",
        "try:\n",
        "    username = reddit.user.me()\n",
        "    print(\"Successfully logged in to Reddit!\")\n",
        "    print(f\"Logged in as: u/{username}\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to log in: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tk6FGRfK8BkS"
      },
      "source": [
        "### Accessing Reddit Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnIgfIks8BkT"
      },
      "source": [
        "To access reddit posts, we'll need send a request with the number of post we want to get. The following example finds the top 10 hottest posts on the u/wallstreetbets subreddit. We'll show the post title, score, flair, and URL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "MMebUrT98BkT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 10 hot posts from r/wallstreetbets:\n",
            "Title: What Are Your Moves Tomorrow, May 02, 2025, Score: 82, Flair: Daily Discussion, URL: https://www.reddit.com/r/wallstreetbets/comments/1kci11n/what_are_your_moves_tomorrow_may_02_2025/\n",
            "Title: Weekly Earnings Thread 4/28 - 5/2, Score: 359, Flair: Earnings Thread, URL: https://i.redd.it/mz7c9szamzwe1.jpeg\n",
            "Title: McDonald’s reports largest U.S. same-store sales decline since 2020, Score: 9205, Flair: News, URL: https://www.cnbc.com/2025/05/01/mcdonalds-mcd-q1-2025-earnings.html\n",
            "Title: A judge just blew up Apple’s control of the App Store, Score: 2161, Flair: News, URL: https://www.theverge.com/news/659246/apple-epic-app-store-judge-ruling-control\n",
            "Title: US weekly jobless claims jump to two-month high, Score: 777, Flair: News, URL: https://www.reuters.com/business/world-at-work/us-weekly-jobless-claims-increase-more-than-expected-2025-05-01/\n",
            "Title: Is no one talking about De Minimis starting tomorrow because of this current short term news?, Score: 389, Flair: Discussion, URL: https://www.wsj.com/articles/e-commerce-sellers-brace-for-end-of-de-minimis-e1a616bf\n",
            "Title: Bears April to May Options Journey, Score: 256, Flair: Meme, URL: https://v.redd.it/c2fs3wmn78ye1\n",
            "Title: RDDT crushed earnings, Score: 197, Flair: News, URL: https://s203.q4cdn.com/380862485/files/doc_news/Reddit-Announces-First-Quarter-2025-Results-2025.pdf\n",
            "Title: Ryanair threatens to seek alternative to Boeing order, Score: 852, Flair: News, URL: https://www.reuters.com/business/aerospace-defense/ryanair-threatens-seek-alternative-boeing-order-if-tariffs-impact-price-2025-05-01/\n",
            "Title: Bang, Score: 402, Flair: Gain, URL: https://i.redd.it/ezgnrau2h6ye1.jpeg\n"
          ]
        }
      ],
      "source": [
        "top_posts = reddit.subreddit('wallstreetbets').hot(limit=10)\n",
        "print(\"Top 10 hot posts from r/wallstreetbets:\")\n",
        "for post in top_posts:\n",
        "    print(f\"Title: {post.title}, Score: {post.score}, Flair: {post.link_flair_text}, URL: {post.url}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IxVa5Xi8BkU"
      },
      "source": [
        "For this project, we'll need far more than ten posts at a time. The reddit API will limit our access to 100 posts at a time. Fortunately, the api uses a ListingGenerator which allows us to access our metered connection in sequential blocks. The following example shows how we can utilize this behavior, grabbing blocks of 100 posts at a time. In our example, we'll grab blocks of posts until we reach 5000 posts or our access times out. Notice that the procedure ends early with around 750-800 posts collected. The results are sparce, because our connection either timed out or was metered down by reddit. The latter option is more likely."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WG2zyxFP8BkU"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Storing batch of posts: 50post [00:00, 88.87post/s]\n",
            "Storing batch of posts: 50post [00:00, 89.47post/s]\n",
            "Storing batch of posts: 50post [00:00, 77.52post/s]\n",
            "Storing batch of posts: 50post [00:00, 68.96post/s]\n",
            "Storing batch of posts: 50post [00:00, 64.40post/s]\n",
            "Storing batch of posts: 50post [00:00, 77.67post/s]\n",
            "Storing batch of posts: 50post [00:00, 70.22post/s]\n",
            "Storing batch of posts: 50post [00:00, 72.35post/s]\n",
            "Storing batch of posts: 50post [00:00, 71.43post/s]\n",
            "Storing batch of posts: 50post [00:00, 76.69post/s]\n",
            "Storing batch of posts: 50post [00:00, 71.03post/s]\n",
            "Storing batch of posts: 50post [00:00, 82.69post/s]\n",
            "Storing batch of posts: 50post [00:00, 80.49post/s]\n",
            "Storing batch of posts: 50post [00:00, 70.36post/s]\n",
            "Storing batch of posts: 50post [00:01, 49.72post/s]\n",
            "Storing batch of posts: 50post [00:01, 25.24post/s]\n",
            "Storing batch of posts: 11post [00:00, 39.05post/s]\n",
            "Storing batch of posts: 0post [00:00, ?post/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No more posts to fetch.\n",
            "Fetched 811 posts in total.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Access the subreddit\n",
        "subreddit = reddit.subreddit(\"wallstreetbets\")\n",
        "\n",
        "# Initialize variables\n",
        "batch_size = 50 # Number of posts per batch\n",
        "total_posts = 5000  # Total number of posts to fetch\n",
        "all_posts = []  # To store all the retrieved posts\n",
        "after = None  # To keep track of the last post for pagination\n",
        "\n",
        "# Fetch posts in batches\n",
        "while len(all_posts) < total_posts:\n",
        "    # Fetch the next batch of posts\n",
        "    submissions = subreddit.new(limit=batch_size, params={\"after\": after})\n",
        "\n",
        "    batch_posts = []\n",
        "    for submission in tqdm(submissions, desc=\"Storing batch of posts\", unit=\"post\"):\n",
        "        batch_posts.append(submission)\n",
        "\n",
        "        # Update the `after` variable with the last submission's fullname\n",
        "        after = submission.fullname\n",
        "\n",
        "    # Add the batch to the main list\n",
        "    all_posts.extend(batch_posts)\n",
        "\n",
        "    # Exit loop if no more posts are available\n",
        "    if not batch_posts:\n",
        "        print(\"No more posts to fetch.\")\n",
        "        break\n",
        "\n",
        "    # Optional delay to avoid rate limits\n",
        "    time.sleep(1)  # Adjust the delay as necessary\n",
        "\n",
        "# Process the data (example: print the total number of posts fetched)\n",
        "print(f\"Fetched {len(all_posts)} posts in total.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_4N4b2q8BkU"
      },
      "source": [
        "Now that we have collected a large portion of posts/submssions, we'll parse the results and construct a dataframe with this data. We're going to collect more fields from this data than we might need right now, avoiding data limitations in the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "69crArfG8BkU"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>created_utc</th>\n",
              "      <th>id</th>\n",
              "      <th>is_original_content</th>\n",
              "      <th>link_flair_text</th>\n",
              "      <th>locked</th>\n",
              "      <th>name</th>\n",
              "      <th>num_comments</th>\n",
              "      <th>over_18</th>\n",
              "      <th>permalink</th>\n",
              "      <th>selftext</th>\n",
              "      <th>spoiler</th>\n",
              "      <th>upvote_ratio</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I call it \"Put Combo\" +44k</td>\n",
              "      <td>1.746135e+09</td>\n",
              "      <td>1kck7mt</td>\n",
              "      <td>False</td>\n",
              "      <td>Gain</td>\n",
              "      <td>False</td>\n",
              "      <td>t3_1kck7mt</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>/r/wallstreetbets/comments/1kck7mt/i_call_it_p...</td>\n",
              "      <td>I call it \"Put Combo\" +44k\\nThe holy grail of ...</td>\n",
              "      <td>False</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Reddit shares rocket as high as 19% on strong ...</td>\n",
              "      <td>1.746135e+09</td>\n",
              "      <td>1kck67g</td>\n",
              "      <td>False</td>\n",
              "      <td>News</td>\n",
              "      <td>False</td>\n",
              "      <td>t3_1kck67g</td>\n",
              "      <td>5</td>\n",
              "      <td>False</td>\n",
              "      <td>/r/wallstreetbets/comments/1kck67g/reddit_shar...</td>\n",
              "      <td></td>\n",
              "      <td>False</td>\n",
              "      <td>0.82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>MSFT earnings</td>\n",
              "      <td>1.746132e+09</td>\n",
              "      <td>1kcj29d</td>\n",
              "      <td>False</td>\n",
              "      <td>Gain</td>\n",
              "      <td>False</td>\n",
              "      <td>t3_1kcj29d</td>\n",
              "      <td>5</td>\n",
              "      <td>False</td>\n",
              "      <td>/r/wallstreetbets/comments/1kcj29d/msft_earnings/</td>\n",
              "      <td>Was down fat on Tsla and MSFT saved my account...</td>\n",
              "      <td>False</td>\n",
              "      <td>0.97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Block shares plunge 17% on revenue miss</td>\n",
              "      <td>1.746132e+09</td>\n",
              "      <td>1kcizwx</td>\n",
              "      <td>False</td>\n",
              "      <td>News</td>\n",
              "      <td>False</td>\n",
              "      <td>t3_1kcizwx</td>\n",
              "      <td>26</td>\n",
              "      <td>False</td>\n",
              "      <td>/r/wallstreetbets/comments/1kcizwx/block_share...</td>\n",
              "      <td></td>\n",
              "      <td>False</td>\n",
              "      <td>0.94</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>AAPL Results</td>\n",
              "      <td>1.746132e+09</td>\n",
              "      <td>1kciy2f</td>\n",
              "      <td>False</td>\n",
              "      <td>Discussion</td>\n",
              "      <td>False</td>\n",
              "      <td>t3_1kciy2f</td>\n",
              "      <td>24</td>\n",
              "      <td>False</td>\n",
              "      <td>/r/wallstreetbets/comments/1kciy2f/aapl_results/</td>\n",
              "      <td>Not so exciting. But new buyback can support p...</td>\n",
              "      <td>False</td>\n",
              "      <td>0.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>806</th>\n",
              "      <td>Largest Net Gain in a Single Day for the Nasda...</td>\n",
              "      <td>1.744231e+09</td>\n",
              "      <td>1jvg6np</td>\n",
              "      <td>False</td>\n",
              "      <td>Discussion</td>\n",
              "      <td>False</td>\n",
              "      <td>t3_1jvg6np</td>\n",
              "      <td>27</td>\n",
              "      <td>False</td>\n",
              "      <td>/r/wallstreetbets/comments/1jvg6np/largest_net...</td>\n",
              "      <td>https://preview.redd.it/72sswb2pevte1.png?widt...</td>\n",
              "      <td>False</td>\n",
              "      <td>0.89</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>807</th>\n",
              "      <td>Make my account green again with $60k gain</td>\n",
              "      <td>1.744231e+09</td>\n",
              "      <td>1jvg693</td>\n",
              "      <td>False</td>\n",
              "      <td>Gain</td>\n",
              "      <td>False</td>\n",
              "      <td>t3_1jvg693</td>\n",
              "      <td>9</td>\n",
              "      <td>False</td>\n",
              "      <td>/r/wallstreetbets/comments/1jvg693/make_my_acc...</td>\n",
              "      <td>Dear regards, I’m back in the game. I lost $60...</td>\n",
              "      <td>False</td>\n",
              "      <td>0.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>808</th>\n",
              "      <td>TQQQ Short Put $1140 within minutes</td>\n",
              "      <td>1.744231e+09</td>\n",
              "      <td>1jvg5fe</td>\n",
              "      <td>False</td>\n",
              "      <td>Gain</td>\n",
              "      <td>False</td>\n",
              "      <td>t3_1jvg5fe</td>\n",
              "      <td>6</td>\n",
              "      <td>False</td>\n",
              "      <td>/r/wallstreetbets/comments/1jvg5fe/tqqq_short_...</td>\n",
              "      <td></td>\n",
              "      <td>False</td>\n",
              "      <td>0.85</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>809</th>\n",
              "      <td>Got lucky. Sold before the pump.</td>\n",
              "      <td>1.744231e+09</td>\n",
              "      <td>1jvg4kf</td>\n",
              "      <td>False</td>\n",
              "      <td>Gain</td>\n",
              "      <td>False</td>\n",
              "      <td>t3_1jvg4kf</td>\n",
              "      <td>6</td>\n",
              "      <td>False</td>\n",
              "      <td>/r/wallstreetbets/comments/1jvg4kf/got_lucky_s...</td>\n",
              "      <td>Been buying puts on the basis that orange man ...</td>\n",
              "      <td>False</td>\n",
              "      <td>0.93</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>810</th>\n",
              "      <td>From $2k -&gt; $108k trading the market swings in...</td>\n",
              "      <td>1.744231e+09</td>\n",
              "      <td>1jvg44a</td>\n",
              "      <td>False</td>\n",
              "      <td>Gain</td>\n",
              "      <td>False</td>\n",
              "      <td>t3_1jvg44a</td>\n",
              "      <td>129</td>\n",
              "      <td>False</td>\n",
              "      <td>/r/wallstreetbets/comments/1jvg44a/from_2k_108...</td>\n",
              "      <td>Trying to post this again because mods said I ...</td>\n",
              "      <td>False</td>\n",
              "      <td>0.96</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>811 rows × 13 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 title   created_utc       id  \\\n",
              "0                           I call it \"Put Combo\" +44k  1.746135e+09  1kck7mt   \n",
              "1    Reddit shares rocket as high as 19% on strong ...  1.746135e+09  1kck67g   \n",
              "2                                        MSFT earnings  1.746132e+09  1kcj29d   \n",
              "3              Block shares plunge 17% on revenue miss  1.746132e+09  1kcizwx   \n",
              "4                                         AAPL Results  1.746132e+09  1kciy2f   \n",
              "..                                                 ...           ...      ...   \n",
              "806  Largest Net Gain in a Single Day for the Nasda...  1.744231e+09  1jvg6np   \n",
              "807         Make my account green again with $60k gain  1.744231e+09  1jvg693   \n",
              "808                TQQQ Short Put $1140 within minutes  1.744231e+09  1jvg5fe   \n",
              "809                   Got lucky. Sold before the pump.  1.744231e+09  1jvg4kf   \n",
              "810  From $2k -> $108k trading the market swings in...  1.744231e+09  1jvg44a   \n",
              "\n",
              "     is_original_content link_flair_text  locked        name  num_comments  \\\n",
              "0                  False            Gain   False  t3_1kck7mt             2   \n",
              "1                  False            News   False  t3_1kck67g             5   \n",
              "2                  False            Gain   False  t3_1kcj29d             5   \n",
              "3                  False            News   False  t3_1kcizwx            26   \n",
              "4                  False      Discussion   False  t3_1kciy2f            24   \n",
              "..                   ...             ...     ...         ...           ...   \n",
              "806                False      Discussion   False  t3_1jvg6np            27   \n",
              "807                False            Gain   False  t3_1jvg693             9   \n",
              "808                False            Gain   False  t3_1jvg5fe             6   \n",
              "809                False            Gain   False  t3_1jvg4kf             6   \n",
              "810                False            Gain   False  t3_1jvg44a           129   \n",
              "\n",
              "     over_18                                          permalink  \\\n",
              "0      False  /r/wallstreetbets/comments/1kck7mt/i_call_it_p...   \n",
              "1      False  /r/wallstreetbets/comments/1kck67g/reddit_shar...   \n",
              "2      False  /r/wallstreetbets/comments/1kcj29d/msft_earnings/   \n",
              "3      False  /r/wallstreetbets/comments/1kcizwx/block_share...   \n",
              "4      False   /r/wallstreetbets/comments/1kciy2f/aapl_results/   \n",
              "..       ...                                                ...   \n",
              "806    False  /r/wallstreetbets/comments/1jvg6np/largest_net...   \n",
              "807    False  /r/wallstreetbets/comments/1jvg693/make_my_acc...   \n",
              "808    False  /r/wallstreetbets/comments/1jvg5fe/tqqq_short_...   \n",
              "809    False  /r/wallstreetbets/comments/1jvg4kf/got_lucky_s...   \n",
              "810    False  /r/wallstreetbets/comments/1jvg44a/from_2k_108...   \n",
              "\n",
              "                                              selftext  spoiler  upvote_ratio  \n",
              "0    I call it \"Put Combo\" +44k\\nThe holy grail of ...    False          1.00  \n",
              "1                                                         False          0.82  \n",
              "2    Was down fat on Tsla and MSFT saved my account...    False          0.97  \n",
              "3                                                         False          0.94  \n",
              "4    Not so exciting. But new buyback can support p...    False          0.96  \n",
              "..                                                 ...      ...           ...  \n",
              "806  https://preview.redd.it/72sswb2pevte1.png?widt...    False          0.89  \n",
              "807  Dear regards, I’m back in the game. I lost $60...    False          0.96  \n",
              "808                                                       False          0.85  \n",
              "809  Been buying puts on the basis that orange man ...    False          0.93  \n",
              "810  Trying to post this again because mods said I ...    False          0.96  \n",
              "\n",
              "[811 rows x 13 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Parse are submission objects that we collected.\n",
        "fields = ('title',\n",
        "          'created_utc',\n",
        "          'id',\n",
        "          'is_original_content',\n",
        "          'link_flair_text',\n",
        "          'locked',\n",
        "          'name',\n",
        "          'num_comments',\n",
        "          'over_18',\n",
        "          'permalink',\n",
        "          'selftext',\n",
        "          'spoiler',\n",
        "          'upvote_ratio')\n",
        "list_of_submissions = []\n",
        "\n",
        "# Parse each submission into a dictionary of the lised fields.\n",
        "for submission in all_posts:\n",
        "    full = vars(submission)\n",
        "    sub_dict = {field:full[field] for field in fields}\n",
        "    list_of_submissions.append(sub_dict)\n",
        "\n",
        "# Create a python dataframe of these submissions.\n",
        "collected_data = pd.DataFrame.from_records(list_of_submissions)\n",
        "\n",
        "# Display the dataframe.\n",
        "display(collected_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWFn6XlM8BkV"
      },
      "source": [
        "### Saving Reddit Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the collected data to parquet format\n",
        "SUBMISSION_PARQUET_PATH = './data/wallstreetbets-collection.parquet'\n",
        "\n",
        "# Create a pyarrow schema for the data types.\n",
        "submission_schema = pyarrow.schema([\n",
        "    ('title', pyarrow.string()),\n",
        "    ('created_utc', pyarrow.float64()),\n",
        "    ('id', pyarrow.string()),\n",
        "    ('is_original_content', pyarrow.bool_()),\n",
        "    ('link_flair_text', pyarrow.string()),\n",
        "    ('locked', pyarrow.bool_()),\n",
        "    ('name', pyarrow.string()),\n",
        "    ('num_comments', pyarrow.int64()),\n",
        "    ('over_18', pyarrow.bool_()),\n",
        "    ('permalink', pyarrow.string()),\n",
        "    ('selftext', pyarrow.string()),\n",
        "    ('spoiler', pyarrow.bool_()),\n",
        "    ('upvote_ratio', pyarrow.float64()),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "leoYaEHt8BkV"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>created_utc</th>\n",
              "      <th>id</th>\n",
              "      <th>is_original_content</th>\n",
              "      <th>link_flair_text</th>\n",
              "      <th>locked</th>\n",
              "      <th>name</th>\n",
              "      <th>num_comments</th>\n",
              "      <th>over_18</th>\n",
              "      <th>permalink</th>\n",
              "      <th>selftext</th>\n",
              "      <th>spoiler</th>\n",
              "      <th>upvote_ratio</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Nivea Along</td>\n",
              "      <td>1.744832e+09</td>\n",
              "      <td>1k0t4jk</td>\n",
              "      <td>False</td>\n",
              "      <td>YOLO</td>\n",
              "      <td>False</td>\n",
              "      <td>t3_1k0t4jk</td>\n",
              "      <td>5</td>\n",
              "      <td>False</td>\n",
              "      <td>/r/wallstreetbets/comments/1k0t4jk/nivea_along/</td>\n",
              "      <td>After -7% yesterday and -10% today</td>\n",
              "      <td>False</td>\n",
              "      <td>0.67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Powell to Volatile Stock Market: You’re on You...</td>\n",
              "      <td>1.744836e+09</td>\n",
              "      <td>1k0unbq</td>\n",
              "      <td>False</td>\n",
              "      <td>News</td>\n",
              "      <td>False</td>\n",
              "      <td>t3_1k0unbq</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>/r/wallstreetbets/comments/1k0unbq/powell_to_v...</td>\n",
              "      <td></td>\n",
              "      <td>False</td>\n",
              "      <td>0.86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Made back the last Wendy’s paycheck I lost</td>\n",
              "      <td>1.744834e+09</td>\n",
              "      <td>1k0tv2y</td>\n",
              "      <td>False</td>\n",
              "      <td>Gain</td>\n",
              "      <td>False</td>\n",
              "      <td>t3_1k0tv2y</td>\n",
              "      <td>6</td>\n",
              "      <td>False</td>\n",
              "      <td>/r/wallstreetbets/comments/1k0tv2y/made_back_t...</td>\n",
              "      <td></td>\n",
              "      <td>False</td>\n",
              "      <td>0.94</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>After market observation. When I finished buyi...</td>\n",
              "      <td>1.744833e+09</td>\n",
              "      <td>1k0tnqx</td>\n",
              "      <td>False</td>\n",
              "      <td>Gain</td>\n",
              "      <td>False</td>\n",
              "      <td>t3_1k0tnqx</td>\n",
              "      <td>8</td>\n",
              "      <td>False</td>\n",
              "      <td>/r/wallstreetbets/comments/1k0tnqx/after_marke...</td>\n",
              "      <td>https://preview.redd.it/41ilvj6f39ve1.png?widt...</td>\n",
              "      <td>False</td>\n",
              "      <td>0.72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Ominous</td>\n",
              "      <td>1.744833e+09</td>\n",
              "      <td>1k0thnd</td>\n",
              "      <td>False</td>\n",
              "      <td>Discussion</td>\n",
              "      <td>False</td>\n",
              "      <td>t3_1k0thnd</td>\n",
              "      <td>110</td>\n",
              "      <td>False</td>\n",
              "      <td>/r/wallstreetbets/comments/1k0thnd/ominous/</td>\n",
              "      <td>NVIDIA 2024 is starting to rhyme like Cisco 20...</td>\n",
              "      <td>False</td>\n",
              "      <td>0.85</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1385</th>\n",
              "      <td>Largest Net Gain in a Single Day for the Nasda...</td>\n",
              "      <td>1.744231e+09</td>\n",
              "      <td>1jvg6np</td>\n",
              "      <td>False</td>\n",
              "      <td>Discussion</td>\n",
              "      <td>False</td>\n",
              "      <td>t3_1jvg6np</td>\n",
              "      <td>27</td>\n",
              "      <td>False</td>\n",
              "      <td>/r/wallstreetbets/comments/1jvg6np/largest_net...</td>\n",
              "      <td>https://preview.redd.it/72sswb2pevte1.png?widt...</td>\n",
              "      <td>False</td>\n",
              "      <td>0.89</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1386</th>\n",
              "      <td>Make my account green again with $60k gain</td>\n",
              "      <td>1.744231e+09</td>\n",
              "      <td>1jvg693</td>\n",
              "      <td>False</td>\n",
              "      <td>Gain</td>\n",
              "      <td>False</td>\n",
              "      <td>t3_1jvg693</td>\n",
              "      <td>9</td>\n",
              "      <td>False</td>\n",
              "      <td>/r/wallstreetbets/comments/1jvg693/make_my_acc...</td>\n",
              "      <td>Dear regards, I’m back in the game. I lost $60...</td>\n",
              "      <td>False</td>\n",
              "      <td>0.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1387</th>\n",
              "      <td>TQQQ Short Put $1140 within minutes</td>\n",
              "      <td>1.744231e+09</td>\n",
              "      <td>1jvg5fe</td>\n",
              "      <td>False</td>\n",
              "      <td>Gain</td>\n",
              "      <td>False</td>\n",
              "      <td>t3_1jvg5fe</td>\n",
              "      <td>6</td>\n",
              "      <td>False</td>\n",
              "      <td>/r/wallstreetbets/comments/1jvg5fe/tqqq_short_...</td>\n",
              "      <td></td>\n",
              "      <td>False</td>\n",
              "      <td>0.85</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1388</th>\n",
              "      <td>Got lucky. Sold before the pump.</td>\n",
              "      <td>1.744231e+09</td>\n",
              "      <td>1jvg4kf</td>\n",
              "      <td>False</td>\n",
              "      <td>Gain</td>\n",
              "      <td>False</td>\n",
              "      <td>t3_1jvg4kf</td>\n",
              "      <td>6</td>\n",
              "      <td>False</td>\n",
              "      <td>/r/wallstreetbets/comments/1jvg4kf/got_lucky_s...</td>\n",
              "      <td>Been buying puts on the basis that orange man ...</td>\n",
              "      <td>False</td>\n",
              "      <td>0.93</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1389</th>\n",
              "      <td>From $2k -&gt; $108k trading the market swings in...</td>\n",
              "      <td>1.744231e+09</td>\n",
              "      <td>1jvg44a</td>\n",
              "      <td>False</td>\n",
              "      <td>Gain</td>\n",
              "      <td>False</td>\n",
              "      <td>t3_1jvg44a</td>\n",
              "      <td>129</td>\n",
              "      <td>False</td>\n",
              "      <td>/r/wallstreetbets/comments/1jvg44a/from_2k_108...</td>\n",
              "      <td>Trying to post this again because mods said I ...</td>\n",
              "      <td>False</td>\n",
              "      <td>0.96</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1390 rows × 13 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  title   created_utc  \\\n",
              "0                                           Nivea Along  1.744832e+09   \n",
              "1     Powell to Volatile Stock Market: You’re on You...  1.744836e+09   \n",
              "2            Made back the last Wendy’s paycheck I lost  1.744834e+09   \n",
              "3     After market observation. When I finished buyi...  1.744833e+09   \n",
              "4                                               Ominous  1.744833e+09   \n",
              "...                                                 ...           ...   \n",
              "1385  Largest Net Gain in a Single Day for the Nasda...  1.744231e+09   \n",
              "1386         Make my account green again with $60k gain  1.744231e+09   \n",
              "1387                TQQQ Short Put $1140 within minutes  1.744231e+09   \n",
              "1388                   Got lucky. Sold before the pump.  1.744231e+09   \n",
              "1389  From $2k -> $108k trading the market swings in...  1.744231e+09   \n",
              "\n",
              "           id  is_original_content link_flair_text  locked        name  \\\n",
              "0     1k0t4jk                False            YOLO   False  t3_1k0t4jk   \n",
              "1     1k0unbq                False            News   False  t3_1k0unbq   \n",
              "2     1k0tv2y                False            Gain   False  t3_1k0tv2y   \n",
              "3     1k0tnqx                False            Gain   False  t3_1k0tnqx   \n",
              "4     1k0thnd                False      Discussion   False  t3_1k0thnd   \n",
              "...       ...                  ...             ...     ...         ...   \n",
              "1385  1jvg6np                False      Discussion   False  t3_1jvg6np   \n",
              "1386  1jvg693                False            Gain   False  t3_1jvg693   \n",
              "1387  1jvg5fe                False            Gain   False  t3_1jvg5fe   \n",
              "1388  1jvg4kf                False            Gain   False  t3_1jvg4kf   \n",
              "1389  1jvg44a                False            Gain   False  t3_1jvg44a   \n",
              "\n",
              "      num_comments  over_18  \\\n",
              "0                5    False   \n",
              "1                2    False   \n",
              "2                6    False   \n",
              "3                8    False   \n",
              "4              110    False   \n",
              "...            ...      ...   \n",
              "1385            27    False   \n",
              "1386             9    False   \n",
              "1387             6    False   \n",
              "1388             6    False   \n",
              "1389           129    False   \n",
              "\n",
              "                                              permalink  \\\n",
              "0       /r/wallstreetbets/comments/1k0t4jk/nivea_along/   \n",
              "1     /r/wallstreetbets/comments/1k0unbq/powell_to_v...   \n",
              "2     /r/wallstreetbets/comments/1k0tv2y/made_back_t...   \n",
              "3     /r/wallstreetbets/comments/1k0tnqx/after_marke...   \n",
              "4           /r/wallstreetbets/comments/1k0thnd/ominous/   \n",
              "...                                                 ...   \n",
              "1385  /r/wallstreetbets/comments/1jvg6np/largest_net...   \n",
              "1386  /r/wallstreetbets/comments/1jvg693/make_my_acc...   \n",
              "1387  /r/wallstreetbets/comments/1jvg5fe/tqqq_short_...   \n",
              "1388  /r/wallstreetbets/comments/1jvg4kf/got_lucky_s...   \n",
              "1389  /r/wallstreetbets/comments/1jvg44a/from_2k_108...   \n",
              "\n",
              "                                               selftext  spoiler  upvote_ratio  \n",
              "0                   After -7% yesterday and -10% today     False          0.67  \n",
              "1                                                          False          0.86  \n",
              "2                                                          False          0.94  \n",
              "3     https://preview.redd.it/41ilvj6f39ve1.png?widt...    False          0.72  \n",
              "4     NVIDIA 2024 is starting to rhyme like Cisco 20...    False          0.85  \n",
              "...                                                 ...      ...           ...  \n",
              "1385  https://preview.redd.it/72sswb2pevte1.png?widt...    False          0.89  \n",
              "1386  Dear regards, I’m back in the game. I lost $60...    False          0.96  \n",
              "1387                                                       False          0.85  \n",
              "1388  Been buying puts on the basis that orange man ...    False          0.93  \n",
              "1389  Trying to post this again because mods said I ...    False          0.96  \n",
              "\n",
              "[1390 rows x 13 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# If the parqet does not exist, create it.\n",
        "if not os.path.exists(SUBMISSION_PARQUET_PATH):\n",
        "    collected_data.to_parquet(SUBMISSION_PARQUET_PATH, engine='pyarrow', schema=submission_schema)\n",
        "\n",
        "# If the data file already exist, merge new data with the existing one.\n",
        "else:\n",
        "    old_parquet = pd.read_parquet(SUBMISSION_PARQUET_PATH, engine='pyarrow', schema=submission_schema)\n",
        "    new_parquet = pd.concat([old_parquet, collected_data])\n",
        "    new_parquet = new_parquet.drop_duplicates(subset=['id','title','created_utc','name','permalink'], keep='last').reset_index(drop=True)\n",
        "    new_parquet.to_parquet(SUBMISSION_PARQUET_PATH, engine='pyarrow', schema=submission_schema)\n",
        "\n",
        "# Use the new collected data to get comment stuff.\n",
        "SUBMISSION_PARQUET_PATH = './data/wallstreetbets-collection.parquet'\n",
        "submission_collection = pd.read_parquet(SUBMISSION_PARQUET_PATH, engine='pyarrow', schema=submission_schema)\n",
        "display(submission_collection)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A52zTdg88BkV"
      },
      "source": [
        "## Part 3: Collecting Comments from Reddit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_K-voA38BkV"
      },
      "source": [
        "### Creating a database of reddit threads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "XXHu7mFu8BkV"
      },
      "outputs": [],
      "source": [
        "# Use the same methofology whih we used to collect submissions, but we'll add a parent submission id. and parent comment id.\n",
        "# Since the comment section can be very deep, we'll limit comments to a breadth of 10.\n",
        "# This may still be a lot more comments than we need for larger discussions.\n",
        "def extract_comments_from_submission(submission_id: str):\n",
        "    try:\n",
        "        submission = reddit.submission(id=submission_id)\n",
        "        submission.comments.replace_more(limit=10)  # Limit to 10 levels of comments\n",
        "        comments = []\n",
        "\n",
        "        for comment in submission.comments.list():\n",
        "            if isinstance(comment, praw.models.MoreComments):\n",
        "                continue\n",
        "\n",
        "            # NOTE: It looks like the top comment may be a user report. We'll ignore if is has certain text.\n",
        "            SKIPTEXT = '**User Report**'\n",
        "            if SKIPTEXT in comment.body:\n",
        "                continue\n",
        "\n",
        "            # Append the comment data to the list\n",
        "            comments.append({\n",
        "                'parent_post_id': submission_id,\n",
        "                'parent_comment_id': comment.parent_id,\n",
        "                'comment_id': comment.id,\n",
        "                'author': str(comment.author),\n",
        "                'created_utc': comment.created_utc,\n",
        "                'score': comment.score,\n",
        "                'body': comment.body\n",
        "            })\n",
        "\n",
        "        return comments\n",
        "\n",
        "    except Exception as e:\n",
        "        # Get the HTTP error code if available\n",
        "        if hasattr(e, 'response') and e.response is not None:\n",
        "            error_code = e.response.status_code\n",
        "            print(f\"HTTP Error {error_code} while fetching comments for submission {submission_id}\")\n",
        "        else:\n",
        "            error_code = None\n",
        "\n",
        "        # Print the an erroor message and return nothing.\n",
        "        print(f\"Error fetching comments for submission {submission_id}: {e}\")\n",
        "        return []\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gYeljFKM8BkV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Submission ID: 1k0t4jk\n",
            "Title: Nivea Along\n",
            "Number of comments: 6\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>parent_post_id</th>\n",
              "      <th>parent_comment_id</th>\n",
              "      <th>comment_id</th>\n",
              "      <th>author</th>\n",
              "      <th>created_utc</th>\n",
              "      <th>score</th>\n",
              "      <th>body</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1k0t4jk</td>\n",
              "      <td>t3_1k0t4jk</td>\n",
              "      <td>mngmxdi</td>\n",
              "      <td>chrissurra</td>\n",
              "      <td>1.744832e+09</td>\n",
              "      <td>4</td>\n",
              "      <td>You mean Neveah</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1k0t4jk</td>\n",
              "      <td>t3_1k0t4jk</td>\n",
              "      <td>mngn956</td>\n",
              "      <td>Alert_Barber_3105</td>\n",
              "      <td>1.744832e+09</td>\n",
              "      <td>6</td>\n",
              "      <td>The skin cream?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1k0t4jk</td>\n",
              "      <td>t3_1k0t4jk</td>\n",
              "      <td>mngoka2</td>\n",
              "      <td>Reasonable_Roger</td>\n",
              "      <td>1.744832e+09</td>\n",
              "      <td>1</td>\n",
              "      <td>Psoriasis calls</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1k0t4jk</td>\n",
              "      <td>t1_mngmxdi</td>\n",
              "      <td>mngnbpk</td>\n",
              "      <td>Own-Foundation3873</td>\n",
              "      <td>1.744832e+09</td>\n",
              "      <td>1</td>\n",
              "      <td>yesss sir</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1k0t4jk</td>\n",
              "      <td>t1_mngn956</td>\n",
              "      <td>mngnj1n</td>\n",
              "      <td>Own-Foundation3873</td>\n",
              "      <td>1.744832e+09</td>\n",
              "      <td>1</td>\n",
              "      <td>pow till it creams</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  parent_post_id parent_comment_id comment_id              author  \\\n",
              "0        1k0t4jk        t3_1k0t4jk    mngmxdi          chrissurra   \n",
              "1        1k0t4jk        t3_1k0t4jk    mngn956   Alert_Barber_3105   \n",
              "2        1k0t4jk        t3_1k0t4jk    mngoka2    Reasonable_Roger   \n",
              "3        1k0t4jk        t1_mngmxdi    mngnbpk  Own-Foundation3873   \n",
              "4        1k0t4jk        t1_mngn956    mngnj1n  Own-Foundation3873   \n",
              "\n",
              "    created_utc  score                body  \n",
              "0  1.744832e+09      4     You mean Neveah  \n",
              "1  1.744832e+09      6     The skin cream?  \n",
              "2  1.744832e+09      1     Psoriasis calls  \n",
              "3  1.744832e+09      1           yesss sir  \n",
              "4  1.744832e+09      1  pow till it creams  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Show the results from one submission's comments\n",
        "submission_id = submission_collection.iloc[0]['id']\n",
        "\n",
        "# How many actual comments are there for this submission?\n",
        "submission = reddit.submission(id=submission_id)\n",
        "print(f\"Submission ID: {submission_id}\")\n",
        "print(f\"Title: {submission.title}\")\n",
        "print(f\"Number of comments: {submission.num_comments}\")\n",
        "\n",
        "# Get the comments for the submission\n",
        "results = extract_comments_from_submission(submission_id)\n",
        "\n",
        "# Create a dataframe of the comments\n",
        "comments_df = pd.DataFrame(results)\n",
        "\n",
        "# Display the comments dataframe\n",
        "display(comments_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "vB3PNPj-8BkV"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching comments for all submissions:   4%|▍         | 62/1390 [00:47<16:56,  1.31submission/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m all_comments \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m submission \u001b[38;5;129;01min\u001b[39;00m tqdm(submission_collection[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m], desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFetching comments for all submissions\u001b[39m\u001b[38;5;124m\"\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubmission\u001b[39m\u001b[38;5;124m\"\u001b[39m):    \n\u001b[0;32m----> 4\u001b[0m     comments \u001b[38;5;241m=\u001b[39m \u001b[43mextract_comments_from_submission\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubmission\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     all_comments\u001b[38;5;241m.\u001b[39mextend(comments)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# time.sleep(1)  # Optional delay to avoid rate limits\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Create a python dataframe of these comments.\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[9], line 7\u001b[0m, in \u001b[0;36mextract_comments_from_submission\u001b[0;34m(submission_id)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      6\u001b[0m     submission \u001b[38;5;241m=\u001b[39m reddit\u001b[38;5;241m.\u001b[39msubmission(\u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39msubmission_id)\n\u001b[0;32m----> 7\u001b[0m     \u001b[43msubmission\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomments\u001b[49m\u001b[38;5;241m.\u001b[39mreplace_more(limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)  \u001b[38;5;66;03m# Limit to 10 levels of comments\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     comments \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m comment \u001b[38;5;129;01min\u001b[39;00m submission\u001b[38;5;241m.\u001b[39mcomments\u001b[38;5;241m.\u001b[39mlist():\n",
            "File \u001b[0;32m~/ProgrammingGlobal/GitHub/machine-learning-stock-prediction/venv/lib/python3.12/site-packages/praw/models/reddit/base.py:38\u001b[0m, in \u001b[0;36mRedditBase.__getattr__\u001b[0;34m(self, attribute)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the value of ``attribute``.\"\"\"\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m attribute\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetched:\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attribute)\n\u001b[1;32m     40\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattribute\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
            "File \u001b[0;32m~/ProgrammingGlobal/GitHub/machine-learning-stock-prediction/venv/lib/python3.12/site-packages/praw/models/reddit/submission.py:726\u001b[0m, in \u001b[0;36mSubmission._fetch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_fetch\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 726\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fetch_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    727\u001b[0m     submission_listing, comment_listing \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m    728\u001b[0m     comment_listing \u001b[38;5;241m=\u001b[39m Listing(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reddit, _data\u001b[38;5;241m=\u001b[39mcomment_listing[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
            "File \u001b[0;32m~/ProgrammingGlobal/GitHub/machine-learning-stock-prediction/venv/lib/python3.12/site-packages/praw/models/reddit/submission.py:744\u001b[0m, in \u001b[0;36mSubmission._fetch_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    742\u001b[0m params\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_additional_fetch_params\u001b[38;5;241m.\u001b[39mcopy())\n\u001b[1;32m    743\u001b[0m path \u001b[38;5;241m=\u001b[39m API_PATH[name]\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfields)\n\u001b[0;32m--> 744\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reddit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/ProgrammingGlobal/GitHub/machine-learning-stock-prediction/venv/lib/python3.12/site-packages/praw/util/deprecate_args.py:46\u001b[0m, in \u001b[0;36m_deprecate_args.<locals>.wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m     arg_string \u001b[38;5;241m=\u001b[39m _generate_arg_string(_old_args[: \u001b[38;5;28mlen\u001b[39m(args)])\n\u001b[1;32m     40\u001b[0m     warn(\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPositional arguments for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m will no longer be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m supported in PRAW 8.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCall this function with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[1;32m     44\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     45\u001b[0m     )\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_old_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/ProgrammingGlobal/GitHub/machine-learning-stock-prediction/venv/lib/python3.12/site-packages/praw/reddit.py:963\u001b[0m, in \u001b[0;36mReddit.request\u001b[0;34m(self, data, files, json, method, params, path)\u001b[0m\n\u001b[1;32m    961\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ClientException(msg)\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_core\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BadRequest \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[1;32m    972\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "File \u001b[0;32m~/ProgrammingGlobal/GitHub/machine-learning-stock-prediction/venv/lib/python3.12/site-packages/prawcore/sessions.py:328\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, path, data, files, json, params, timeout)\u001b[0m\n\u001b[1;32m    326\u001b[0m     json[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    327\u001b[0m url \u001b[38;5;241m=\u001b[39m urljoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_requestor\u001b[38;5;241m.\u001b[39moauth_url, path)\n\u001b[0;32m--> 328\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_with_retries\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/ProgrammingGlobal/GitHub/machine-learning-stock-prediction/venv/lib/python3.12/site-packages/prawcore/sessions.py:234\u001b[0m, in \u001b[0;36mSession._request_with_retries\u001b[0;34m(self, data, files, json, method, params, timeout, url, retry_strategy_state)\u001b[0m\n\u001b[1;32m    232\u001b[0m retry_strategy_state\u001b[38;5;241m.\u001b[39msleep()\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_request(data, method, params, url)\n\u001b[0;32m--> 234\u001b[0m response, saved_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry_strategy_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m do_retry \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m codes[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munauthorized\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
            "File \u001b[0;32m~/ProgrammingGlobal/GitHub/machine-learning-stock-prediction/venv/lib/python3.12/site-packages/prawcore/sessions.py:186\u001b[0m, in \u001b[0;36mSession._make_request\u001b[0;34m(self, data, files, json, method, params, retry_strategy_state, timeout, url)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_make_request\u001b[39m(\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    176\u001b[0m     data: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    183\u001b[0m     url: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    184\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[Response, \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m]:\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 186\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rate_limiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_requestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_header_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m            \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m    199\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResponse: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m bytes) (rst-\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:rem-\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:used-\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m ratelimit) at \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    200\u001b[0m             response\u001b[38;5;241m.\u001b[39mstatus_code,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    205\u001b[0m             time\u001b[38;5;241m.\u001b[39mtime(),\n\u001b[1;32m    206\u001b[0m         )\n\u001b[1;32m    207\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/ProgrammingGlobal/GitHub/machine-learning-stock-prediction/venv/lib/python3.12/site-packages/prawcore/rate_limit.py:47\u001b[0m, in \u001b[0;36mRateLimiter.call\u001b[0;34m(self, request_function, set_header_callback, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelay()\n\u001b[1;32m     46\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m set_header_callback()\n\u001b[0;32m---> 47\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequest_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(response\u001b[38;5;241m.\u001b[39mheaders)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
            "File \u001b[0;32m~/ProgrammingGlobal/GitHub/machine-learning-stock-prediction/venv/lib/python3.12/site-packages/prawcore/requestor.py:68\u001b[0m, in \u001b[0;36mRequestor.request\u001b[0;34m(self, timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Issue the HTTP request capturing any errors that may occur.\"\"\"\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_http\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:  \u001b[38;5;66;03m# noqa: BLE001\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RequestException(exc, args, kwargs) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/ProgrammingGlobal/GitHub/machine-learning-stock-prediction/venv/lib/python3.12/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
            "File \u001b[0;32m~/ProgrammingGlobal/GitHub/machine-learning-stock-prediction/venv/lib/python3.12/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
            "File \u001b[0;32m~/ProgrammingGlobal/GitHub/machine-learning-stock-prediction/venv/lib/python3.12/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
            "File \u001b[0;32m~/ProgrammingGlobal/GitHub/machine-learning-stock-prediction/venv/lib/python3.12/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[0;32m~/ProgrammingGlobal/GitHub/machine-learning-stock-prediction/venv/lib/python3.12/site-packages/urllib3/connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
            "File \u001b[0;32m~/ProgrammingGlobal/GitHub/machine-learning-stock-prediction/venv/lib/python3.12/site-packages/urllib3/connection.py:516\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    513\u001b[0m _shutdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    515\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    519\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
            "File \u001b[0;32m/usr/lib/python3.12/http/client.py:1428\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1427\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1428\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1429\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1430\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
            "File \u001b[0;32m/usr/lib/python3.12/http/client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[0;32m/usr/lib/python3.12/http/client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/usr/lib/python3.12/socket.py:707\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 707\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    709\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[0;32m/usr/lib/python3.12/ssl.py:1252\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1250\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1251\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
            "File \u001b[0;32m/usr/lib/python3.12/ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1105\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1106\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Collect the comments for all the submissions.\n",
        "all_comments = []\n",
        "for submission in tqdm(submission_collection['id'], desc=\"Fetching comments for all submissions\", unit=\"submission\"):    \n",
        "    comments = extract_comments_from_submission(submission)\n",
        "    all_comments.extend(comments)\n",
        "    # time.sleep(1)  # Optional delay to avoid rate limits\n",
        "\n",
        "# Create a python dataframe of these comments.\n",
        "comments_df = pd.DataFrame.from_records(all_comments)\n",
        "display(comments_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the collected data to parquet format\n",
        "COMMENT_PARQUET_PATH = './data/wallstreetbets-comment-collection.parquet'\n",
        "\n",
        "# Create a pyarrow schema for the comment data\n",
        "comment_schema = pyarrow.schema([\n",
        "    ('parent_post_id', pyarrow.string()),\n",
        "    ('parent_comment_id', pyarrow.string()),\n",
        "    ('comment_id', pyarrow.string()),\n",
        "    ('author', pyarrow.string()),\n",
        "    ('created_utc', pyarrow.float64()),\n",
        "    ('score', pyarrow.int64()),\n",
        "    ('body', pyarrow.string())\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNVWhQB_8BkW"
      },
      "outputs": [],
      "source": [
        "# Write the comments to parquet file. If it exists, append to it.\n",
        "if not os.path.exists(COMMENT_PARQUET_PATH):\n",
        "    comments_df.to_parquet(COMMENT_PARQUET_PATH, engine='pyarrow', schema=comment_schema)\n",
        "else:\n",
        "    old_parquet = pd.read_parquet(COMMENT_PARQUET_PATH, engine='pyarrow', schema=comment_schema)\n",
        "    new_parquet = pd.concat([old_parquet, comments_df])\n",
        "    new_parquet = new_parquet.drop_duplicates(subset=['parent_post_id','parent_comment_id','author','created_utc'], keep='last').reset_index(drop=True)\n",
        "    new_parquet.to_parquet(COMMENT_PARQUET_PATH, engine='pyarrow', schema=comment_schema)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 4: Collecting More Data (w/ Kaggle)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Read in the new reddit data from kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/gpreda/reddit-wallstreetsbets-posts\n"
          ]
        }
      ],
      "source": [
        "# Download path for the kaggle dataset.\n",
        "KAGGLE_DATASET_PATH = './data/kaggle-dataset'\n",
        "\n",
        "kaggle.api.authenticate()\n",
        "kaggle.api.dataset_download_files('gpreda/reddit-wallstreetsbets-posts', path='./data/', unzip=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>score</th>\n",
              "      <th>id</th>\n",
              "      <th>url</th>\n",
              "      <th>comms_num</th>\n",
              "      <th>created</th>\n",
              "      <th>body</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>It's not about the money, it's about sending a...</td>\n",
              "      <td>55</td>\n",
              "      <td>l6ulcx</td>\n",
              "      <td>https://v.redd.it/6j75regs72e61</td>\n",
              "      <td>6</td>\n",
              "      <td>1.611863e+09</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Math Professor Scott Steiner says the numbers ...</td>\n",
              "      <td>110</td>\n",
              "      <td>l6uibd</td>\n",
              "      <td>https://v.redd.it/ah50lyny62e61</td>\n",
              "      <td>23</td>\n",
              "      <td>1.611862e+09</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Exit the system</td>\n",
              "      <td>0</td>\n",
              "      <td>l6uhhn</td>\n",
              "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
              "      <td>47</td>\n",
              "      <td>1.611862e+09</td>\n",
              "      <td>The CEO of NASDAQ pushed to halt trading “to g...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NEW SEC FILING FOR GME! CAN SOMEONE LESS RETAR...</td>\n",
              "      <td>29</td>\n",
              "      <td>l6ugk6</td>\n",
              "      <td>https://sec.report/Document/0001193125-21-019848/</td>\n",
              "      <td>74</td>\n",
              "      <td>1.611862e+09</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Not to distract from GME, just thought our AMC...</td>\n",
              "      <td>71</td>\n",
              "      <td>l6ufgy</td>\n",
              "      <td>https://i.redd.it/4h2sukb662e61.jpg</td>\n",
              "      <td>156</td>\n",
              "      <td>1.611862e+09</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53182</th>\n",
              "      <td>What I Learned Investigating SAVA FUD Spreaders</td>\n",
              "      <td>238</td>\n",
              "      <td>owd2pn</td>\n",
              "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
              "      <td>87</td>\n",
              "      <td>1.627906e+09</td>\n",
              "      <td>***TLDR: Three bitter scientists partnered up ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53183</th>\n",
              "      <td>Daily Popular Tickers Thread for August 02, 20...</td>\n",
              "      <td>228</td>\n",
              "      <td>owd1a5</td>\n",
              "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
              "      <td>1070</td>\n",
              "      <td>1.627906e+09</td>\n",
              "      <td>Your daily hype thread. Please keep the shitp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53184</th>\n",
              "      <td>Hitler reacts to the market being irrational</td>\n",
              "      <td>7398</td>\n",
              "      <td>owc5dr</td>\n",
              "      <td>https://v.redd.it/46jxu074exe71</td>\n",
              "      <td>372</td>\n",
              "      <td>1.627902e+09</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53185</th>\n",
              "      <td>Daily Discussion Thread for August 02, 2021</td>\n",
              "      <td>338</td>\n",
              "      <td>owbfjf</td>\n",
              "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
              "      <td>11688</td>\n",
              "      <td>1.627898e+09</td>\n",
              "      <td>Your daily trading discussion thread. Please k...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53186</th>\n",
              "      <td>Fraternal Association of Gambling Gentlemen an...</td>\n",
              "      <td>40</td>\n",
              "      <td>owaqd6</td>\n",
              "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
              "      <td>810</td>\n",
              "      <td>1.627895e+09</td>\n",
              "      <td>This is an old Yacht Club thread. Click /u/Vis...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>53187 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   title  score      id  \\\n",
              "0      It's not about the money, it's about sending a...     55  l6ulcx   \n",
              "1      Math Professor Scott Steiner says the numbers ...    110  l6uibd   \n",
              "2                                        Exit the system      0  l6uhhn   \n",
              "3      NEW SEC FILING FOR GME! CAN SOMEONE LESS RETAR...     29  l6ugk6   \n",
              "4      Not to distract from GME, just thought our AMC...     71  l6ufgy   \n",
              "...                                                  ...    ...     ...   \n",
              "53182    What I Learned Investigating SAVA FUD Spreaders    238  owd2pn   \n",
              "53183  Daily Popular Tickers Thread for August 02, 20...    228  owd1a5   \n",
              "53184       Hitler reacts to the market being irrational   7398  owc5dr   \n",
              "53185        Daily Discussion Thread for August 02, 2021    338  owbfjf   \n",
              "53186  Fraternal Association of Gambling Gentlemen an...     40  owaqd6   \n",
              "\n",
              "                                                     url  comms_num  \\\n",
              "0                        https://v.redd.it/6j75regs72e61          6   \n",
              "1                        https://v.redd.it/ah50lyny62e61         23   \n",
              "2      https://www.reddit.com/r/wallstreetbets/commen...         47   \n",
              "3      https://sec.report/Document/0001193125-21-019848/         74   \n",
              "4                    https://i.redd.it/4h2sukb662e61.jpg        156   \n",
              "...                                                  ...        ...   \n",
              "53182  https://www.reddit.com/r/wallstreetbets/commen...         87   \n",
              "53183  https://www.reddit.com/r/wallstreetbets/commen...       1070   \n",
              "53184                    https://v.redd.it/46jxu074exe71        372   \n",
              "53185  https://www.reddit.com/r/wallstreetbets/commen...      11688   \n",
              "53186  https://www.reddit.com/r/wallstreetbets/commen...        810   \n",
              "\n",
              "            created                                               body  \n",
              "0      1.611863e+09                                               <NA>  \n",
              "1      1.611862e+09                                               <NA>  \n",
              "2      1.611862e+09  The CEO of NASDAQ pushed to halt trading “to g...  \n",
              "3      1.611862e+09                                               <NA>  \n",
              "4      1.611862e+09                                               <NA>  \n",
              "...             ...                                                ...  \n",
              "53182  1.627906e+09  ***TLDR: Three bitter scientists partnered up ...  \n",
              "53183  1.627906e+09  \n",
              "Your daily hype thread. Please keep the shitp...  \n",
              "53184  1.627902e+09                                               <NA>  \n",
              "53185  1.627898e+09  Your daily trading discussion thread. Please k...  \n",
              "53186  1.627895e+09  This is an old Yacht Club thread. Click /u/Vis...  \n",
              "\n",
              "[53187 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Path to kaggle dataset.\n",
        "KAGGLE_DATASET_RAW_PATH = './data/reddit_wsb.csv'\n",
        "\n",
        "\n",
        "# Read the kaggle dataset.\n",
        "kaggle_df = pd.read_csv(KAGGLE_DATASET_RAW_PATH)\n",
        "\n",
        "# Drop the timestamp column if it exists.\n",
        "if 'timestamp' in kaggle_df.columns:\n",
        "    kaggle_df = kaggle_df.drop(columns=['timestamp'])\n",
        "\n",
        "# Enforce the schema.\n",
        "kaggle_df = kaggle_df.astype({\n",
        "    'title': 'string',\n",
        "    'score': 'int64',\n",
        "    'id': 'string',\n",
        "    'url': 'string',\n",
        "    'comms_num': 'int64',\n",
        "    'created': 'float64',\n",
        "    'body': 'string',\n",
        "})\n",
        "\n",
        "# Display the kaggle dataset.\n",
        "display(kaggle_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mapping to previous schema.\n",
        "kaggle_mapping = {\n",
        "    'title': 'title',\n",
        "    'score': 'upvote_ratio',\n",
        "    'id': 'id',\n",
        "    'url': 'permalink',\n",
        "    'comms_num': 'num_comments',\n",
        "    'created': 'created_utc',\n",
        "    'body': 'selftext'\n",
        "}\n",
        "\n",
        "# Rename the columns to match our schema.\n",
        "kaggle_df.rename(columns=kaggle_mapping, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Path to kaggle dataset final.\n",
        "KAGGLE_DATASET_FINAL_PATH = './data/kaggle-reddit-wsb.parquet'\n",
        "\n",
        "# Schema for the kaggle dataset.\n",
        "kaggle_schema = pyarrow.schema([\n",
        "    ('title', pyarrow.string()),\n",
        "    ('upvote_ratio', pyarrow.int64()),\n",
        "    ('id', pyarrow.string()),\n",
        "    ('permalink', pyarrow.string()),\n",
        "    ('num_comments', pyarrow.int64()),\n",
        "    ('created_utc', pyarrow.float64()),\n",
        "    ('selftext', pyarrow.string()),\n",
        "])\n",
        "\n",
        "# write the dataframe to parquet.\n",
        "kaggle_df.to_parquet(KAGGLE_DATASET_FINAL_PATH, engine='pyarrow', schema=kaggle_schema)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Merge the new data table with the old data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read in the datasets.\n",
        "praw_dataset = pd.read_parquet(SUBMISSION_PARQUET_PATH, engine='pyarrow', schema=submission_schema)\n",
        "kaggle_dataset = pd.read_parquet(KAGGLE_DATASET_FINAL_PATH, engine='pyarrow', schema=kaggle_schema)\n",
        "\n",
        "# Create similar columns with only the columns of the kaggle dataset.\n",
        "praw_dataset = praw_dataset[['title', 'upvote_ratio', 'id', 'permalink', 'num_comments', 'created_utc', 'selftext']]\n",
        "kaggle_dataset = kaggle_dataset[['title', 'upvote_ratio', 'id', 'permalink', 'num_comments', 'created_utc', 'selftext']]\n",
        "\n",
        "# Use the mapping to merge the two datasets.\n",
        "merged_dataset = pd.concat([praw_dataset, kaggle_dataset], ignore_index=True)\n",
        "\n",
        "# Remove duplicates based on the 'id' column.\n",
        "merged_dataset = merged_dataset.drop_duplicates(subset=['id'], keep='last').reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creare a final schema for the merged dataset.\n",
        "merged_schema = pyarrow.schema([\n",
        "    ('title', pyarrow.string()),\n",
        "    ('upvote_ratio', pyarrow.float64()),\n",
        "    ('id', pyarrow.string()),\n",
        "    ('permalink', pyarrow.string()),\n",
        "    ('num_comments', pyarrow.int64()),\n",
        "    ('created_utc', pyarrow.float64()),\n",
        "    ('selftext', pyarrow.string())\n",
        "])\n",
        "\n",
        "# Save the merged dataset to parquet format.\n",
        "MERGED_DATASET_PATH = './data/merged-reddit-wsb.parquet'\n",
        "merged_dataset.to_parquet(MERGED_DATASET_PATH, engine='pyarrow', schema=merged_schema)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 54577 entries, 0 to 54576\n",
            "Data columns (total 7 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   title         54577 non-null  object \n",
            " 1   upvote_ratio  54577 non-null  float64\n",
            " 2   id            54577 non-null  object \n",
            " 3   permalink     54577 non-null  object \n",
            " 4   num_comments  54577 non-null  int64  \n",
            " 5   created_utc   54577 non-null  float64\n",
            " 6   selftext      26128 non-null  object \n",
            "dtypes: float64(2), int64(1), object(4)\n",
            "memory usage: 2.9+ MB\n",
            "--------------------------------------------------\n",
            "Merged dataset saved to: ./data/merged-reddit-wsb.parquet\n",
            "Number of columns in merged dataset: 7\n",
            "Columns in merged dataset: ['title', 'upvote_ratio', 'id', 'permalink', 'num_comments', 'created_utc', 'selftext']\n",
            "--------------------------------------------------\n",
            "Number of rows in praw dataset: 1390\n",
            "Number of rows in kaggle dataset: 53187\n",
            "Number of rows in merged dataset: 54577\n"
          ]
        }
      ],
      "source": [
        "# Display properties of the merged dataset.\n",
        "merged_dataset.info()\n",
        "print(\"-\" * 50)\n",
        "print(\"Merged dataset saved to:\", MERGED_DATASET_PATH)\n",
        "print(\"Number of columns in merged dataset:\", len(merged_dataset.columns))\n",
        "print(\"Columns in merged dataset:\", merged_dataset.columns.tolist())\n",
        "print(\"-\" * 50)\n",
        "print(\"Number of rows in praw dataset:\", len(praw_dataset))\n",
        "print(\"Number of rows in kaggle dataset:\", len(kaggle_dataset))\n",
        "print(\"Number of rows in merged dataset:\", len(merged_dataset))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
