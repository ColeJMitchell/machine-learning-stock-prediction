{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94356631",
   "metadata": {},
   "source": [
    "# Adding Sentiment Scores to Reddit Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff67acb8",
   "metadata": {},
   "source": [
    "## Part 0: Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8b3f16",
   "metadata": {},
   "source": [
    "#### Setup basic utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5560a078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "import pyarrow, os, re\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Check if running in Google Colab\n",
    "def is_colab():\n",
    "    from IPython import get_ipython\n",
    "    return get_ipython().__class__.__module__ == \"google.colab._shell\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ac5ed6",
   "metadata": {},
   "source": [
    "#### Setup NLTK (Natural Language Tool Kit) utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe1fd9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to ./NLTK_DATA...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to ./NLTK_DATA...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to ./NLTK_DATA...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to ./NLTK_DATA...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package names to ./NLTK_DATA...\n",
      "[nltk_data]   Package names is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     ./NLTK_DATA...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to ./NLTK_DATA...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Set the NLTK data directory\n",
    "NLTK_DOWNLOAD_DIR = './NLTK_DATA'\n",
    "os.environ[\"NLTK_DATA\"]=NLTK_DOWNLOAD_DIR\n",
    "\n",
    "# Then, import NLTK and download the necessary data.\n",
    "import nltk\n",
    "\n",
    "# Do not download this data without understanding the implications.\n",
    "nltk.download(['punkt',\n",
    "               'punkt_tab',\n",
    "               'stopwords',\n",
    "               'vader_lexicon',\n",
    "               'names',\n",
    "               'averaged_perceptron_tagger',\n",
    "               'wordnet'], download_dir=NLTK_DOWNLOAD_DIR)\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd730c0c",
   "metadata": {},
   "source": [
    "## Part 1: Read Collected Reddit Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5055d3f",
   "metadata": {},
   "source": [
    "Submission Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e6d2a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the collected data to parquet format\n",
    "SUBMISSION_PARQUET_PATH = './data/wallstreetbets-collection.parquet'\n",
    "\n",
    "# Verify that the path exists\n",
    "if not os.path.exists(SUBMISSION_PARQUET_PATH):\n",
    "    print(f\"Error: The file {SUBMISSION_PARQUET_PATH} does not exist.\")\n",
    "    \n",
    "# Create a pyarrow schema for the data types.\n",
    "submission_schema = pyarrow.schema([\n",
    "    ('title', pyarrow.string()),\n",
    "    ('created_utc', pyarrow.float64()),\n",
    "    ('id', pyarrow.string()),\n",
    "    ('is_original_content', pyarrow.bool_()),\n",
    "    ('link_flair_text', pyarrow.string()),\n",
    "    ('locked', pyarrow.bool_()),\n",
    "    ('name', pyarrow.string()),\n",
    "    ('num_comments', pyarrow.int64()),\n",
    "    ('over_18', pyarrow.bool_()),\n",
    "    ('permalink', pyarrow.string()),\n",
    "    ('selftext', pyarrow.string()),\n",
    "    ('spoiler', pyarrow.bool_()),\n",
    "    ('upvote_ratio', pyarrow.float64()),\n",
    "])\n",
    "\n",
    "submission_collection = pd.read_parquet(SUBMISSION_PARQUET_PATH, engine='pyarrow', schema=submission_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44ee7652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>id</th>\n",
       "      <th>is_original_content</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>locked</th>\n",
       "      <th>name</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>over_18</th>\n",
       "      <th>permalink</th>\n",
       "      <th>selftext</th>\n",
       "      <th>spoiler</th>\n",
       "      <th>upvote_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nivea Along</td>\n",
       "      <td>1.744832e+09</td>\n",
       "      <td>1k0t4jk</td>\n",
       "      <td>False</td>\n",
       "      <td>YOLO</td>\n",
       "      <td>False</td>\n",
       "      <td>t3_1k0t4jk</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>/r/wallstreetbets/comments/1k0t4jk/nivea_along/</td>\n",
       "      <td>After -7% yesterday and -10% today</td>\n",
       "      <td>False</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Powell to Volatile Stock Market: You’re on You...</td>\n",
       "      <td>1.744836e+09</td>\n",
       "      <td>1k0unbq</td>\n",
       "      <td>False</td>\n",
       "      <td>News</td>\n",
       "      <td>False</td>\n",
       "      <td>t3_1k0unbq</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>/r/wallstreetbets/comments/1k0unbq/powell_to_v...</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Made back the last Wendy’s paycheck I lost</td>\n",
       "      <td>1.744834e+09</td>\n",
       "      <td>1k0tv2y</td>\n",
       "      <td>False</td>\n",
       "      <td>Gain</td>\n",
       "      <td>False</td>\n",
       "      <td>t3_1k0tv2y</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>/r/wallstreetbets/comments/1k0tv2y/made_back_t...</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>After market observation. When I finished buyi...</td>\n",
       "      <td>1.744833e+09</td>\n",
       "      <td>1k0tnqx</td>\n",
       "      <td>False</td>\n",
       "      <td>Gain</td>\n",
       "      <td>False</td>\n",
       "      <td>t3_1k0tnqx</td>\n",
       "      <td>8</td>\n",
       "      <td>False</td>\n",
       "      <td>/r/wallstreetbets/comments/1k0tnqx/after_marke...</td>\n",
       "      <td>https://preview.redd.it/41ilvj6f39ve1.png?widt...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ominous</td>\n",
       "      <td>1.744833e+09</td>\n",
       "      <td>1k0thnd</td>\n",
       "      <td>False</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>False</td>\n",
       "      <td>t3_1k0thnd</td>\n",
       "      <td>110</td>\n",
       "      <td>False</td>\n",
       "      <td>/r/wallstreetbets/comments/1k0thnd/ominous/</td>\n",
       "      <td>NVIDIA 2024 is starting to rhyme like Cisco 20...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title   created_utc       id  \\\n",
       "0                                        Nivea Along  1.744832e+09  1k0t4jk   \n",
       "1  Powell to Volatile Stock Market: You’re on You...  1.744836e+09  1k0unbq   \n",
       "2         Made back the last Wendy’s paycheck I lost  1.744834e+09  1k0tv2y   \n",
       "3  After market observation. When I finished buyi...  1.744833e+09  1k0tnqx   \n",
       "4                                            Ominous  1.744833e+09  1k0thnd   \n",
       "\n",
       "   is_original_content link_flair_text  locked        name  num_comments  \\\n",
       "0                False            YOLO   False  t3_1k0t4jk             5   \n",
       "1                False            News   False  t3_1k0unbq             2   \n",
       "2                False            Gain   False  t3_1k0tv2y             6   \n",
       "3                False            Gain   False  t3_1k0tnqx             8   \n",
       "4                False      Discussion   False  t3_1k0thnd           110   \n",
       "\n",
       "   over_18                                          permalink  \\\n",
       "0    False    /r/wallstreetbets/comments/1k0t4jk/nivea_along/   \n",
       "1    False  /r/wallstreetbets/comments/1k0unbq/powell_to_v...   \n",
       "2    False  /r/wallstreetbets/comments/1k0tv2y/made_back_t...   \n",
       "3    False  /r/wallstreetbets/comments/1k0tnqx/after_marke...   \n",
       "4    False        /r/wallstreetbets/comments/1k0thnd/ominous/   \n",
       "\n",
       "                                            selftext  spoiler  upvote_ratio  \n",
       "0                After -7% yesterday and -10% today     False          0.67  \n",
       "1                                                       False          0.86  \n",
       "2                                                       False          0.94  \n",
       "3  https://preview.redd.it/41ilvj6f39ve1.png?widt...    False          0.72  \n",
       "4  NVIDIA 2024 is starting to rhyme like Cisco 20...    False          0.85  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the first few rows of the submission collection.\n",
    "display(submission_collection.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192fc2ec",
   "metadata": {},
   "source": [
    "Comment Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a64cea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the collected data to parquet format\n",
    "COMMENT_PARQUET_PATH = './data/wallstreetbets-comment-collection.parquet'\n",
    "\n",
    "# Verify that the path exists\n",
    "if not os.path.exists(COMMENT_PARQUET_PATH):\n",
    "    print(f\"Error: The file {COMMENT_PARQUET_PATH} does not exist.\")\n",
    "    \n",
    "# Create a pyarrow schema for the comment data\n",
    "comment_schema = pyarrow.schema([\n",
    "    ('parent_post_id', pyarrow.string()),\n",
    "    ('parent_comment_id', pyarrow.string()),\n",
    "    ('comment_id', pyarrow.string()),\n",
    "    ('author', pyarrow.string()),\n",
    "    ('created_utc', pyarrow.float64()),\n",
    "    ('score', pyarrow.int64()),\n",
    "    ('body', pyarrow.string())\n",
    "])\n",
    "\n",
    "comment_collection = pd.read_parquet(COMMENT_PARQUET_PATH, engine='pyarrow', schema=comment_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd799451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parent_post_id</th>\n",
       "      <th>parent_comment_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>score</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1jwqbs7</td>\n",
       "      <td>t1_mmq5ys9</td>\n",
       "      <td>mmr2q1q</td>\n",
       "      <td>JazzlikePackage5128</td>\n",
       "      <td>1.744474e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>Ty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1jwqbs7</td>\n",
       "      <td>t1_mmumxfs</td>\n",
       "      <td>mn0wa66</td>\n",
       "      <td>shmoopdoop6969</td>\n",
       "      <td>1.744615e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>why</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1jwqbs7</td>\n",
       "      <td>t1_mn0gfl4</td>\n",
       "      <td>mnavkz2</td>\n",
       "      <td>diggin-the-doge</td>\n",
       "      <td>1.744752e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>I take it all back. Tim Dillon special just re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1jwqbs7</td>\n",
       "      <td>t1_mmxdo0h</td>\n",
       "      <td>mmzwh9l</td>\n",
       "      <td>Hugheston987</td>\n",
       "      <td>1.744597e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>![img](emote|t5_2th52|58355)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1jwqbs7</td>\n",
       "      <td>t1_mmplbah</td>\n",
       "      <td>mmsxb6r</td>\n",
       "      <td>markHart99</td>\n",
       "      <td>1.744496e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>![img](emote|t5_2th52|4258)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  parent_post_id parent_comment_id comment_id               author  \\\n",
       "0        1jwqbs7        t1_mmq5ys9    mmr2q1q  JazzlikePackage5128   \n",
       "1        1jwqbs7        t1_mmumxfs    mn0wa66       shmoopdoop6969   \n",
       "2        1jwqbs7        t1_mn0gfl4    mnavkz2      diggin-the-doge   \n",
       "3        1jwqbs7        t1_mmxdo0h    mmzwh9l         Hugheston987   \n",
       "4        1jwqbs7        t1_mmplbah    mmsxb6r           markHart99   \n",
       "\n",
       "    created_utc  score                                               body  \n",
       "0  1.744474e+09      1                                                 Ty  \n",
       "1  1.744615e+09      1                                                why  \n",
       "2  1.744752e+09      1  I take it all back. Tim Dillon special just re...  \n",
       "3  1.744597e+09      1                       ![img](emote|t5_2th52|58355)  \n",
       "4  1.744496e+09      1                        ![img](emote|t5_2th52|4258)  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the first few rows of the comment collection.\n",
    "display(comment_collection.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abc4dc7",
   "metadata": {},
   "source": [
    "Merged Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29ab2348",
   "metadata": {},
   "outputs": [],
   "source": [
    "MERGED_DATASET_PATH = './data/merged-reddit-wsb.parquet'\n",
    "\n",
    "# Verify that the path exists\n",
    "if not os.path.exists(MERGED_DATASET_PATH):\n",
    "    print(f\"Error: The file {MERGED_DATASET_PATH} does not exist.\")\n",
    "\n",
    "# Create a pyarrow schema for the merged dataset.\n",
    "merged_schema = pyarrow.schema([\n",
    "    ('title', pyarrow.string()),\n",
    "    ('upvote_ratio', pyarrow.float64()),\n",
    "    ('id', pyarrow.string()),\n",
    "    ('permalink', pyarrow.string()),\n",
    "    ('num_comments', pyarrow.int64()),\n",
    "    ('created_utc', pyarrow.float64()),\n",
    "    ('selftext', pyarrow.string())\n",
    "])\n",
    "\n",
    "merged_collection = pd.read_parquet(MERGED_DATASET_PATH, engine='pyarrow', schema=merged_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b75618",
   "metadata": {},
   "source": [
    "## Part 2: Initial Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ac17c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Sentiment Intensity Analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to analyze sentiment of a single comment\n",
    "def analyze_sentiment(comment):\n",
    "    # Tokenize the comment\n",
    "    tokens = word_tokenize(comment.lower())\n",
    "\n",
    "    # Remove stop words\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Get sentiment scores\n",
    "    sentiment_scores = sia.polarity_scores(' '.join(filtered_tokens))\n",
    "\n",
    "    return sentiment_scores\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove special characters\n",
    "    text = re.sub(r'\\@\\w+|\\#', '', text)\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68508d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission Details:\n",
      "Dumping submission details: {'title': 'Nivea Along', 'created_utc': 1744831733.0, 'id': '1k0t4jk', 'is_original_content': False, 'link_flair_text': 'YOLO', 'locked': False, 'name': 't3_1k0t4jk', 'num_comments': 5, 'over_18': False, 'permalink': '/r/wallstreetbets/comments/1k0t4jk/nivea_along/', 'selftext': 'After -7% yesterday and -10% today ', 'spoiler': False, 'upvote_ratio': 0.67}\n",
      "--------------------------------------------------------------------------------\n",
      "Submission: Nivea Along\n",
      "Sentiment Scores: {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Get the first submssion from the collection\n",
    "submission = submission_collection.iloc[0]\n",
    "print(f\"Submission Details:\")\n",
    "print(f\"Dumping submission details: {submission.to_dict()}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Show the results of the analysis.\n",
    "sentiment_scores = analyze_sentiment(submission.title)\n",
    "print(f\"Submission: {submission.title}\")\n",
    "print(f\"Sentiment Scores: {sentiment_scores}\")\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be2438a",
   "metadata": {},
   "source": [
    "## Part 3: Sentiment Score for Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28731bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding sentiment scores to the submission collection...\n",
      "Provided the analysis for fields: selftext.\n",
      "--------------------------------------------------------------------------------\n",
      "Sentiment scores added to the submission collection.\n",
      "--------------------------------------------------------------------------------\n",
      "Example of the submission collection with sentiment scores:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nivea Along</td>\n",
       "      <td>After -7% yesterday and -10% today</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Powell to Volatile Stock Market: You’re on You...</td>\n",
       "      <td></td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Made back the last Wendy’s paycheck I lost</td>\n",
       "      <td></td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>After market observation. When I finished buyi...</td>\n",
       "      <td>https://preview.redd.it/41ilvj6f39ve1.png?widt...</td>\n",
       "      <td>{'neg': 0.034, 'neu': 0.882, 'pos': 0.085, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ominous</td>\n",
       "      <td>NVIDIA 2024 is starting to rhyme like Cisco 20...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.687, 'pos': 0.313, 'comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Monday Boeing, Tuesday Nivida, Today AMD, Tomo...</td>\n",
       "      <td>Intel got a charge of 900 million to sell CPUs...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.946, 'pos': 0.054, 'comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Before and After</td>\n",
       "      <td>Repost to make it pg, though I think my origin...</td>\n",
       "      <td>{'neg': 0.084, 'neu': 0.727, 'pos': 0.189, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Before, After, and Dr*gs?</td>\n",
       "      <td>My before and after with MSTR. Current overall...</td>\n",
       "      <td>{'neg': 0.097, 'neu': 0.591, 'pos': 0.312, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Uranium Yolo</td>\n",
       "      <td></td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>IBKR $185 C 6/20. What are your thoughts regar...</td>\n",
       "      <td>Asking for a friend.</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.238, 'pos': 0.762, 'comp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                                        Nivea Along   \n",
       "1  Powell to Volatile Stock Market: You’re on You...   \n",
       "2         Made back the last Wendy’s paycheck I lost   \n",
       "3  After market observation. When I finished buyi...   \n",
       "4                                            Ominous   \n",
       "5  Monday Boeing, Tuesday Nivida, Today AMD, Tomo...   \n",
       "6                                   Before and After   \n",
       "7                          Before, After, and Dr*gs?   \n",
       "8                                       Uranium Yolo   \n",
       "9  IBKR $185 C 6/20. What are your thoughts regar...   \n",
       "\n",
       "                                            selftext  \\\n",
       "0                After -7% yesterday and -10% today    \n",
       "1                                                      \n",
       "2                                                      \n",
       "3  https://preview.redd.it/41ilvj6f39ve1.png?widt...   \n",
       "4  NVIDIA 2024 is starting to rhyme like Cisco 20...   \n",
       "5  Intel got a charge of 900 million to sell CPUs...   \n",
       "6  Repost to make it pg, though I think my origin...   \n",
       "7  My before and after with MSTR. Current overall...   \n",
       "8                                                      \n",
       "9                               Asking for a friend.   \n",
       "\n",
       "                                           sentiment  \n",
       "0  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...  \n",
       "1  {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound...  \n",
       "2  {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound...  \n",
       "3  {'neg': 0.034, 'neu': 0.882, 'pos': 0.085, 'co...  \n",
       "4  {'neg': 0.0, 'neu': 0.687, 'pos': 0.313, 'comp...  \n",
       "5  {'neg': 0.0, 'neu': 0.946, 'pos': 0.054, 'comp...  \n",
       "6  {'neg': 0.084, 'neu': 0.727, 'pos': 0.189, 'co...  \n",
       "7  {'neg': 0.097, 'neu': 0.591, 'pos': 0.312, 'co...  \n",
       "8  {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound...  \n",
       "9  {'neg': 0.0, 'neu': 0.238, 'pos': 0.762, 'comp...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Parse the sentiment scores into separate columns.\n",
      "Add four columns to the submission collection.\n",
      "Sentiment scores parsed into separate columns.\n",
      "--------------------------------------------------------------------------------\n",
      "Add a predicted (ss_) to the sentiment scores columns.\n",
      "Sentiment scores columns renamed.\n",
      "--------------------------------------------------------------------------------\n",
      "Example of the submission collection with sentiment scores parsed into separate columns:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>ss_neg</th>\n",
       "      <th>ss_neu</th>\n",
       "      <th>ss_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nivea Along</td>\n",
       "      <td>After -7% yesterday and -10% today</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Powell to Volatile Stock Market: You’re on You...</td>\n",
       "      <td></td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Made back the last Wendy’s paycheck I lost</td>\n",
       "      <td></td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>After market observation. When I finished buyi...</td>\n",
       "      <td>https://preview.redd.it/41ilvj6f39ve1.png?widt...</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.882</td>\n",
       "      <td>0.085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ominous</td>\n",
       "      <td>NVIDIA 2024 is starting to rhyme like Cisco 20...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.687</td>\n",
       "      <td>0.313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Monday Boeing, Tuesday Nivida, Today AMD, Tomo...</td>\n",
       "      <td>Intel got a charge of 900 million to sell CPUs...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.946</td>\n",
       "      <td>0.054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Before and After</td>\n",
       "      <td>Repost to make it pg, though I think my origin...</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.727</td>\n",
       "      <td>0.189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Before, After, and Dr*gs?</td>\n",
       "      <td>My before and after with MSTR. Current overall...</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.591</td>\n",
       "      <td>0.312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Uranium Yolo</td>\n",
       "      <td></td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>IBKR $185 C 6/20. What are your thoughts regar...</td>\n",
       "      <td>Asking for a friend.</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.238</td>\n",
       "      <td>0.762</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                                        Nivea Along   \n",
       "1  Powell to Volatile Stock Market: You’re on You...   \n",
       "2         Made back the last Wendy’s paycheck I lost   \n",
       "3  After market observation. When I finished buyi...   \n",
       "4                                            Ominous   \n",
       "5  Monday Boeing, Tuesday Nivida, Today AMD, Tomo...   \n",
       "6                                   Before and After   \n",
       "7                          Before, After, and Dr*gs?   \n",
       "8                                       Uranium Yolo   \n",
       "9  IBKR $185 C 6/20. What are your thoughts regar...   \n",
       "\n",
       "                                            selftext  ss_neg  ss_neu  ss_pos  \n",
       "0                After -7% yesterday and -10% today    0.000   1.000   0.000  \n",
       "1                                                      0.000   0.000   0.000  \n",
       "2                                                      0.000   0.000   0.000  \n",
       "3  https://preview.redd.it/41ilvj6f39ve1.png?widt...   0.034   0.882   0.085  \n",
       "4  NVIDIA 2024 is starting to rhyme like Cisco 20...   0.000   0.687   0.313  \n",
       "5  Intel got a charge of 900 million to sell CPUs...   0.000   0.946   0.054  \n",
       "6  Repost to make it pg, though I think my origin...   0.084   0.727   0.189  \n",
       "7  My before and after with MSTR. Current overall...   0.097   0.591   0.312  \n",
       "8                                                      0.000   0.000   0.000  \n",
       "9                               Asking for a friend.   0.000   0.238   0.762  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add the sentiment scores to the submission collection.\n",
    "print(\"Adding sentiment scores to the submission collection...\")\n",
    "print(\"Provided the analysis for fields: selftext.\")\n",
    "print(\"-\" * 80)\n",
    "submission_collection['sentiment'] = submission_collection['selftext'].apply(analyze_sentiment)\n",
    "print(\"Sentiment scores added to the submission collection.\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Example of the submission collection with sentiment scores:\")\n",
    "display(submission_collection[['title', 'selftext', 'sentiment']].head(10))\n",
    "print(\"-\" * 80)\n",
    "print(\"Parse the sentiment scores into separate columns.\")\n",
    "print(\"Add four columns to the submission collection.\")\n",
    "submission_collection[['neg', 'neu', 'pos', 'compound']] = submission_collection['sentiment'].apply(pd.Series)\n",
    "print(\"Sentiment scores parsed into separate columns.\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Add a predicted (ss_) to the sentiment scores columns.\")\n",
    "submission_collection.rename(columns={'neg': 'ss_neg', 'neu': 'ss_neu', 'pos': 'ss_pos', 'compound': 'ss_compound'}, inplace=True)\n",
    "print(\"Sentiment scores columns renamed.\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Example of the submission collection with sentiment scores parsed into separate columns:\")\n",
    "display(submission_collection[['title', 'selftext', 'ss_neg', 'ss_neu', 'ss_pos']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e0b573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding sentiment scores to the comment collection...\n",
      "Provided the analysis for fields: body.\n",
      "--------------------------------------------------------------------------------\n",
      "Sentiment scores added to the comment collection.\n",
      "--------------------------------------------------------------------------------\n",
      "Example of the comment collection with sentiment scores:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ty</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>why</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I take it all back. Tim Dillon special just re...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.69, 'pos': 0.31, 'compou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>![img](emote|t5_2th52|58355)</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>![img](emote|t5_2th52|4258)</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>yes</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Little retards are legitimately in shock ![img...</td>\n",
       "      <td>{'neg': 0.345, 'neu': 0.655, 'pos': 0.0, 'comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Feels like a horrible time to look for a diffe...</td>\n",
       "      <td>{'neg': 0.424, 'neu': 0.424, 'pos': 0.152, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AMD and NVDA are down 10% ![img](emote|t5_2th5...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>This is a classic Powell talks and shit hits t...</td>\n",
       "      <td>{'neg': 0.211, 'neu': 0.409, 'pos': 0.38, 'com...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body  \\\n",
       "0                                                 Ty   \n",
       "1                                                why   \n",
       "2  I take it all back. Tim Dillon special just re...   \n",
       "3                       ![img](emote|t5_2th52|58355)   \n",
       "4                        ![img](emote|t5_2th52|4258)   \n",
       "5                                                yes   \n",
       "6  Little retards are legitimately in shock ![img...   \n",
       "7  Feels like a horrible time to look for a diffe...   \n",
       "8  AMD and NVDA are down 10% ![img](emote|t5_2th5...   \n",
       "9  This is a classic Powell talks and shit hits t...   \n",
       "\n",
       "                                           sentiment  \n",
       "0  {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound...  \n",
       "1  {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound...  \n",
       "2  {'neg': 0.0, 'neu': 0.69, 'pos': 0.31, 'compou...  \n",
       "3  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...  \n",
       "4  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...  \n",
       "5  {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound...  \n",
       "6  {'neg': 0.345, 'neu': 0.655, 'pos': 0.0, 'comp...  \n",
       "7  {'neg': 0.424, 'neu': 0.424, 'pos': 0.152, 'co...  \n",
       "8  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...  \n",
       "9  {'neg': 0.211, 'neu': 0.409, 'pos': 0.38, 'com...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Parse the sentiment scores into separate columns.\n",
      "Add four columns to the comment collection.\n",
      "Sentiment scores parsed into separate columns.\n",
      "--------------------------------------------------------------------------------\n",
      "Add a predicted (ss_) to the sentiment scores columns.\n",
      "Sentiment scores columns renamed.\n",
      "--------------------------------------------------------------------------------\n",
      "Example of the comment collection with sentiment scores parsed into separate columns:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>ss_neg</th>\n",
       "      <th>ss_neu</th>\n",
       "      <th>ss_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ty</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>why</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I take it all back. Tim Dillon special just re...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>![img](emote|t5_2th52|58355)</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>![img](emote|t5_2th52|4258)</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>yes</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Little retards are legitimately in shock ![img...</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Feels like a horrible time to look for a diffe...</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AMD and NVDA are down 10% ![img](emote|t5_2th5...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>This is a classic Powell talks and shit hits t...</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.409</td>\n",
       "      <td>0.380</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body  ss_neg  ss_neu  ss_pos\n",
       "0                                                 Ty   0.000   0.000   1.000\n",
       "1                                                why   0.000   0.000   0.000\n",
       "2  I take it all back. Tim Dillon special just re...   0.000   0.690   0.310\n",
       "3                       ![img](emote|t5_2th52|58355)   0.000   1.000   0.000\n",
       "4                        ![img](emote|t5_2th52|4258)   0.000   1.000   0.000\n",
       "5                                                yes   0.000   0.000   1.000\n",
       "6  Little retards are legitimately in shock ![img...   0.345   0.655   0.000\n",
       "7  Feels like a horrible time to look for a diffe...   0.424   0.424   0.152\n",
       "8  AMD and NVDA are down 10% ![img](emote|t5_2th5...   0.000   1.000   0.000\n",
       "9  This is a classic Powell talks and shit hits t...   0.211   0.409   0.380"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add the sentiment scores to the comment collection.\n",
    "print(\"Adding sentiment scores to the comment collection...\")\n",
    "print(\"Provided the analysis for fields: body.\")\n",
    "print(\"-\" * 80)\n",
    "comment_collection['sentiment'] = comment_collection['body'].apply(analyze_sentiment)\n",
    "print(\"Sentiment scores added to the comment collection.\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Example of the comment collection with sentiment scores:\")\n",
    "display(comment_collection[['body', 'sentiment']].head(10))\n",
    "print(\"-\" * 80)\n",
    "print(\"Parse the sentiment scores into separate columns.\")\n",
    "print(\"Add four columns to the comment collection.\")\n",
    "comment_collection[['neg', 'neu', 'pos', 'compound']] = comment_collection['sentiment'].apply(pd.Series)\n",
    "print(\"Sentiment scores parsed into separate columns.\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Add a predicted (ss_) to the sentiment scores columns.\")\n",
    "comment_collection.rename(columns={'neg': 'ss_neg', 'neu': 'ss_neu', 'pos': 'ss_pos', 'compound': 'ss_compound'}, inplace=True)\n",
    "print(\"Sentiment scores columns renamed.\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Example of the comment collection with sentiment scores parsed into separate columns:\")\n",
    "display(comment_collection[['body', 'ss_neg', 'ss_neu', 'ss_pos']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "397d622c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding sentiment scores to the merged collection...\n",
      "Provided the analysis for fields: title.\n",
      "--------------------------------------------------------------------------------\n",
      "Sentiment scores added to the merged collection.\n",
      "--------------------------------------------------------------------------------\n",
      "Example of the merged collection with sentiment scores:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title+selftext</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0                                             ...</td>\n",
       "      <td>{'neg': 0.12, 'neu': 0.805, 'pos': 0.076, 'com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0                                             ...</td>\n",
       "      <td>{'neg': 0.12, 'neu': 0.805, 'pos': 0.076, 'com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0                                             ...</td>\n",
       "      <td>{'neg': 0.12, 'neu': 0.805, 'pos': 0.076, 'com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0                                             ...</td>\n",
       "      <td>{'neg': 0.12, 'neu': 0.805, 'pos': 0.076, 'com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0                                             ...</td>\n",
       "      <td>{'neg': 0.12, 'neu': 0.805, 'pos': 0.076, 'com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0                                             ...</td>\n",
       "      <td>{'neg': 0.12, 'neu': 0.805, 'pos': 0.076, 'com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0                                             ...</td>\n",
       "      <td>{'neg': 0.12, 'neu': 0.805, 'pos': 0.076, 'com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0                                             ...</td>\n",
       "      <td>{'neg': 0.12, 'neu': 0.805, 'pos': 0.076, 'com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0                                             ...</td>\n",
       "      <td>{'neg': 0.12, 'neu': 0.805, 'pos': 0.076, 'com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0                                             ...</td>\n",
       "      <td>{'neg': 0.12, 'neu': 0.805, 'pos': 0.076, 'com...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      title+selftext  \\\n",
       "0  0                                             ...   \n",
       "1  0                                             ...   \n",
       "2  0                                             ...   \n",
       "3  0                                             ...   \n",
       "4  0                                             ...   \n",
       "5  0                                             ...   \n",
       "6  0                                             ...   \n",
       "7  0                                             ...   \n",
       "8  0                                             ...   \n",
       "9  0                                             ...   \n",
       "\n",
       "                                           sentiment  \n",
       "0  {'neg': 0.12, 'neu': 0.805, 'pos': 0.076, 'com...  \n",
       "1  {'neg': 0.12, 'neu': 0.805, 'pos': 0.076, 'com...  \n",
       "2  {'neg': 0.12, 'neu': 0.805, 'pos': 0.076, 'com...  \n",
       "3  {'neg': 0.12, 'neu': 0.805, 'pos': 0.076, 'com...  \n",
       "4  {'neg': 0.12, 'neu': 0.805, 'pos': 0.076, 'com...  \n",
       "5  {'neg': 0.12, 'neu': 0.805, 'pos': 0.076, 'com...  \n",
       "6  {'neg': 0.12, 'neu': 0.805, 'pos': 0.076, 'com...  \n",
       "7  {'neg': 0.12, 'neu': 0.805, 'pos': 0.076, 'com...  \n",
       "8  {'neg': 0.12, 'neu': 0.805, 'pos': 0.076, 'com...  \n",
       "9  {'neg': 0.12, 'neu': 0.805, 'pos': 0.076, 'com...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Parse the sentiment scores into separate columns.\n",
      "Add four columns to the merged collection.\n",
      "Sentiment scores parsed into separate columns.\n",
      "--------------------------------------------------------------------------------\n",
      "Add a predicted (ss_) to the sentiment scores columns.\n",
      "Sentiment scores columns renamed.\n",
      "--------------------------------------------------------------------------------\n",
      "Example of the merged collection with sentiment scores parsed into separate columns:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title+selftext</th>\n",
       "      <th>ss_neg</th>\n",
       "      <th>ss_neg</th>\n",
       "      <th>ss_neg</th>\n",
       "      <th>ss_neu</th>\n",
       "      <th>ss_neu</th>\n",
       "      <th>ss_neu</th>\n",
       "      <th>ss_pos</th>\n",
       "      <th>ss_pos</th>\n",
       "      <th>ss_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0                                             ...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0                                             ...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0                                             ...</td>\n",
       "      <td>0.315</td>\n",
       "      <td>0.315</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.685</td>\n",
       "      <td>0.685</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0                                             ...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0                                             ...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0                                             ...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0                                             ...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0                                             ...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0                                             ...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.677</td>\n",
       "      <td>0.677</td>\n",
       "      <td>0.076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0                                             ...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.076</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      title+selftext  ss_neg  ss_neg  ss_neg  \\\n",
       "0  0                                             ...   0.000   0.000    0.12   \n",
       "1  0                                             ...   0.000   0.000    0.12   \n",
       "2  0                                             ...   0.315   0.315    0.12   \n",
       "3  0                                             ...   0.000   0.000    0.12   \n",
       "4  0                                             ...   1.000   1.000    0.12   \n",
       "5  0                                             ...   0.000   0.000    0.12   \n",
       "6  0                                             ...   0.000   0.000    0.12   \n",
       "7  0                                             ...   0.000   0.000    0.12   \n",
       "8  0                                             ...   0.000   0.000    0.12   \n",
       "9  0                                             ...   0.000   0.000    0.12   \n",
       "\n",
       "   ss_neu  ss_neu  ss_neu  ss_pos  ss_pos  ss_pos  \n",
       "0   1.000   1.000   0.805   0.000   0.000   0.076  \n",
       "1   1.000   1.000   0.805   0.000   0.000   0.076  \n",
       "2   0.685   0.685   0.805   0.000   0.000   0.076  \n",
       "3   1.000   1.000   0.805   0.000   0.000   0.076  \n",
       "4   0.000   0.000   0.805   0.000   0.000   0.076  \n",
       "5   1.000   1.000   0.805   0.000   0.000   0.076  \n",
       "6   0.000   0.000   0.805   0.000   0.000   0.076  \n",
       "7   1.000   1.000   0.805   0.000   0.000   0.076  \n",
       "8   0.323   0.323   0.805   0.677   0.677   0.076  \n",
       "9   1.000   1.000   0.805   0.000   0.000   0.076  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add the sentiment scores to the submission collection.\n",
    "print(\"Adding sentiment scores to the merged collection...\")\n",
    "print(\"Provided the analysis for fields: title+selftext.\")\n",
    "print(\"-\" * 80)\n",
    "merged_collection['title+selftext'] = f'{merged_collection['title']} {merged_collection['selftext']}'\n",
    "merged_collection['sentiment'] = merged_collection['title+selftext'].apply(analyze_sentiment)\n",
    "print(\"Sentiment scores added to the merged collection.\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Example of the merged collection with sentiment scores:\")\n",
    "display(merged_collection[['title+selftext', 'sentiment']].head(10))\n",
    "print(\"-\" * 80)\n",
    "print(\"Parse the sentiment scores into separate columns.\")\n",
    "print(\"Add four columns to the merged collection.\")\n",
    "merged_collection[['neg', 'neu', 'pos', 'compound']] = merged_collection['sentiment'].apply(pd.Series)\n",
    "print(\"Sentiment scores parsed into separate columns.\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Add a predicted (ss_) to the sentiment scores columns.\")\n",
    "merged_collection.rename(columns={'neg': 'ss_neg', 'neu': 'ss_neu', 'pos': 'ss_pos', 'compound': 'ss_compound'}, inplace=True)\n",
    "print(\"Sentiment scores columns renamed.\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Example of the merged collection with sentiment scores parsed into separate columns:\")\n",
    "display(merged_collection[['title+selftext', 'ss_neg', 'ss_neu', 'ss_pos']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a7353a",
   "metadata": {},
   "source": [
    "## Part 6: Update Reddit Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba6a49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the collected data to parquet format\n",
    "COMMENT_PARQUET_PATH = './data/wallstreetbets-comment-collection-wss.parquet'\n",
    "\n",
    "# Create a pyarrow schema for the comment data\n",
    "comment_schema = pyarrow.schema([\n",
    "    ('parent_post_id', pyarrow.string()),\n",
    "    ('parent_comment_id', pyarrow.string()),\n",
    "    ('comment_id', pyarrow.string()),\n",
    "    ('author', pyarrow.string()),\n",
    "    ('created_utc', pyarrow.float64()),\n",
    "    ('score', pyarrow.int64()),\n",
    "    ('body', pyarrow.string()),\n",
    "    ('ss_neg', pyarrow.float64()),\n",
    "    ('ss_neu', pyarrow.float64()),\n",
    "    ('ss_pos', pyarrow.float64()),\n",
    "    ('ss_compound', pyarrow.float64())\n",
    "])\n",
    "\n",
    "# Save the collected data to parquet format\n",
    "comment_collection.to_parquet(COMMENT_PARQUET_PATH, engine='pyarrow', schema=comment_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519aa318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the collected data to parquet format\n",
    "SUBMISSION_PARQUET_PATH = './data/wallstreetbets-collection-wss.parquet'\n",
    "\n",
    "# Create a pyarrow schema for the data types.\n",
    "submission_schema = pyarrow.schema([\n",
    "    ('title', pyarrow.string()),\n",
    "    ('created_utc', pyarrow.float64()),\n",
    "    ('id', pyarrow.string()),\n",
    "    ('is_original_content', pyarrow.bool_()),\n",
    "    ('link_flair_text', pyarrow.string()),\n",
    "    ('locked', pyarrow.bool_()),\n",
    "    ('name', pyarrow.string()),\n",
    "    ('num_comments', pyarrow.int64()),\n",
    "    ('over_18', pyarrow.bool_()),\n",
    "    ('permalink', pyarrow.string()),\n",
    "    ('selftext', pyarrow.string()),\n",
    "    ('spoiler', pyarrow.bool_()),\n",
    "    ('upvote_ratio', pyarrow.float64()),\n",
    "    ('ss_neg', pyarrow.float64()),\n",
    "    ('ss_neu', pyarrow.float64()),\n",
    "    ('ss_pos', pyarrow.float64()),\n",
    "    ('ss_compound', pyarrow.float64())\n",
    "])\n",
    "\n",
    "# Save the collected data to parquet format\n",
    "submission_collection.to_parquet(SUBMISSION_PARQUET_PATH, engine='pyarrow', schema=submission_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca267d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MERGED_DATASET_PATH = './data/merged-reddit-wsb-wss.parquet'\n",
    "\n",
    "# Create a pyarrow schema for the merged dataset.\n",
    "merged_schema = pyarrow.schema([\n",
    "    ('title', pyarrow.string()),\n",
    "    ('upvote_ratio', pyarrow.float64()),\n",
    "    ('id', pyarrow.string()),\n",
    "    ('permalink', pyarrow.string()),\n",
    "    ('num_comments', pyarrow.int64()),\n",
    "    ('created_utc', pyarrow.float64()),\n",
    "    ('selftext', pyarrow.string()),\n",
    "    ('ss_neg', pyarrow.float64()),\n",
    "    ('ss_neu', pyarrow.float64()),\n",
    "    ('ss_pos', pyarrow.float64()),\n",
    "    ('ss_compound', pyarrow.float64())\n",
    "])\n",
    "\n",
    "merged_collection = pd.read_parquet(MERGED_DATASET_PATH, engine='pyarrow', schema=merged_schema)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
